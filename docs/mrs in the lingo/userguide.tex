\documentclass[12pt]{article}

\usepackage{avm}
\usepackage{lingmacros}
\usepackage{robbib}
\usepackage{rtrees}
\usepackage{fullpage}
\usepackage{pstricks}
\usepackage{tree-dvips}

\newcommand{\itsdb}{{\sf \lbrack incr tsdb()\rbrack}}
\newcommand{\hpsg}{\textsc{hpsg}}
\newcommand{\tsnlp}{\textsc{tsnlp}}
\newcommand{\pet}{\textsc{pet}}
\newcommand{\cheap}{\textsf{cheap}}
\newcommand{\tdl}{\mbox{${\cal T\!D\!L}$}}
\newcommand{\lingo}{{LinGO}}
\newcommand{\erg}{\textsc{erg}}
\newcommand{\viz}{viz.}
\newcommand{\ie}{i.e.}
\newcommand{\vs}{vs.}
\newcommand{\eg}{e.g.}
\newcommand{\qeq}{\textsc{qeq}}
\newcommand{\lkb}{\textsc{lkb}}
\newcommand{\mrs}{MRS}
\newcommand{\ale}{\textsc{ale}}
\newcommand{\tfs}{\textsc{tfs}}
\newcommand{\lfg}{\textsc{lfg}}
\newcommand{\xle}{\textsc{xle}}
\newcommand{\page}{\textsc{page}}
\newcommand{\ptb}{\textsc{ptb}}
\newcommand{\negra}{NeGra}
\newcommand{\tiger}{TiGer}
\newcommand{\testset}[1]{`\hspace*{-0.25ex}{\it #1}\/'}
\newcommand{\hpt}{$\cdot$}
\newcommand{\verbmobil}{VerbMobil}

\newcommand{\avmplus}[1]{{\setlength{\arraycolsep}{0.8mm}	
                       \renewcommand{\arraystretch}{1.1} %1.2
                       \left[ 			
                       \begin{array}{l}
                       \\[-2mm] #1 \\[-2mm] \\
                       \end{array} 		
                       \right]
                    }}
%
\newcommand{\att}[1]{{\mbox{\scriptsize {\bf #1}}}}
\newcommand{\attval}[2]{{\mbox{\scriptsize #1}\ {{#2}}}}
\newcommand{\attvallist}[2]{{\mbox{\scriptsize #1}\ {<{#2}>}}}
\newcommand{\attvaltlist}[2]{{\mbox{\scriptsize #1}\ {<{\myvaluebold{#2}}>}}}
\newcommand{\attvaltyp}[2]{{\mbox{\scriptsize #1}\ {\myvaluebold{#2}}}}
\newcommand{\attvalshrunktyp}[2]{{\mbox{\scriptsize #1}\ {\boxvaluebold{#2}}}}
\newcommand{\myvaluebold}[1]{{\mbox{\scriptsize {\bf #1}}}}
\newcommand{\boxvaluebold}[1]{{\fbox{\scriptsize {\bf #1}}}}
\newcommand{\ind}[1]{{\setlength{\fboxsep}{0.25mm} \: \fbox{{\small #1}} \:}}
\newcommand{\mpt}[1]{\marginpar{\tiny #1}}


\avmfont{\small\sc}
\avmvalfont{\footnotesize\it}

\newcommand{\es}{\enumsentence}
\newcommand{\ees}{\eenumsentence}
\newcommand{\fn}{\footnote}
\newcommand{\mc}{\multicolumn}
\newcommand{\tn}{\textnormal}
\newcommand{\ncon}{\nodeconnect}

\title{MRS in the LinGO Grammar Matrix:\\ A Practical User's Guide}
\author{Dan Flickinger, Emily M. Bender, and Stephan Oepen}
\date{2007-08-03: Minor changes, lots of marginal notes}

\begin{document}
\maketitle

\setcounter{tocdepth}{2}  % 2 = include subsections in table of contents
\tableofcontents
\newpage
\section{Introduction}

This paper is intended to serve as documentation for the semantic
aspects of the LinGO Grammar Matrix. We assume familiarity with the
specification of Minimal Recursion Semantics (MRS) given in
\namecite{Cop:Fli:Sag:Pol:03} (see also \citeboth{Cop:Las:Fli:01}),
and focus instead on the practical aspects of writing grammars
that produce well-formed {\it mrs}s and of developing semantic
representations of particular linguistic phenomena within MRS.

Section \ref{backmot} gives some background on the LinGO Grammar
Matrix and MRS.  Sections \ref{basicobj}--\ref{synsem-i} describe
the implementation of MRS in the Matrix and how to extend it in
building a grammar of a specific language.  Along the way, we
provide touchstone linguistic examples and advice on best practice.
Section \ref{verif} describes tools
and methods for verifying the well-formedness of {\it mrs}s.  Finally,
Section \ref{semisec} describes the Sem-I (`Semantic Interface'), a component
of the grammar that specifies how {\it mrs}s are to be interpreted.

\section{Background and Motivation}
\label{backmot}

The Matrix grammar starter-kit
\cite{Ben:Fli:Oep:02,Fli:Ben:03,Ben:Fli:05,Dre:Ben:05} is a
language-independent core grammar designed to facilitate the rapid
initial development of grammars for natural languages, with
foundations solid enough to support steady expansion to broad coverage
of the linguistic phenomena in these languages.  Such grammars are
particularly valuable because they can assign semantic representations
to linguistic input, providing the foundation for applications which
require natural language understanding.  As such, a central component
of the Matrix is the collection of resources it contains for
simplifying the implementation of semantic composition within each
language and supporting the development of a standardized description
language for meaning representations, which can provide an effective
interface for practical applications.  The resources in the Matrix
further enable the meaning representations to keep pace as the
syntactic analyses of a grammar grow in complexity.

The goal of the Matrix grammar starter-kit is to provide the necessary\mpt{Update to reflect customization system}
definitions of core linguistic types for words and phrases at a level
of generality which enables quick specialization to encode the
additional basic grammatical constraints for a particular language.
With this language-specific tuning, it should be possible to construct
a grammar within an afternoon which can be used to parse non-trivial
sentences of a given language, then use that same implementation as
the basis for the development over time of a semantically precise,
broad-coverage grammar.  The existing Matrix release also includes
software links and parameter settings for one particular grammar
development system, the LKB \cite{Copestake:02}, which includes an
efficient parser and generator, but grammars built on the Matrix can
be read and used by a number of other parsers
(cf.~\citeboth{Oep:Fli:Tsu:02}).

The Matrix is constructed within the formal system of typed feature
structures defined in \citeboth{Carpenter:92}, using the single operation
of unification to build phrases from the words and phrases they
contain.  Minimal Recursion Semantics was designed to enable semantic
composition using only this same unification of typed feature
structures, producing for each phrase or sentence a description of the
meaning representation sufficient to support logical inference.  The
type definitions for signs in the Matrix include a semantic component
which is an implementation of MRS, and more specifically of the
elaboration of a semantic algebra for MRS presented in
\namecite{Cop:Las:Fli:01}.  In addition, MRS was designed to answer the
competing demands of expressive adequacy and computational
tractability, as well as to allow underspecification where it
facilitates computational applications, such as machine translation.
Thus Matrix-derived grammars are not only interesting as a means of
testing linguistic hypotheses, but also have the potential to be
integrated in applications which require natural language
understanding, including machine translation, automated email response
and speech prostheses.

Minimal Recursion Semantics is not a theory of semantics but rather a
system of semantic representations.  As such, in order to develop a
grammar of a particular language, there are any number of design
decisions that must be made about the content of the semantic
representations.  In this paper, we present some design decisions that
have emerged in work on two broad-coverage grammars the English
Resource Grammar (ERG: \citeboth{Flickinger00}) and the JACY Japanese
grammar (JACY: \citeboth{Sie:Ben:02}).\mpt{This will be replaced with 
broader discussion of where these representations come from, more references
to projects involved, more languages.}
While the influences on these design decisions
are myriad, perhaps the most important guiding principle is the
following: The semantic representations produced should include all
grammatically relevant distinctions, while remaining as concise as
possible.  Thus, for example, in the Matrix as in the ERG, past tense
is represented simply as the attribute-value pair \mbox{[TENSE past]}\mpt{Read \& cite Goss-Grubbs MA thesis}
(on an event variable) rather than a more elaborate Reichenbachian
representation relating the event time to the time of utterance,
because this more elaborate representation can be unambiguously
derived from the more parsimonious one given.  (For more discussion,
see \S\ref{AgrTAM} below and {\S}6 of \citeboth{Cop:Fli:Sag:Pol:03}.)
We hope that the descriptions in the following sections of the design
decisions taken so far will provide guidance for grammar engineers
confronting new linguistic phenomena.

\section{Basic Semantic Objects}
\label{basicobj}

This section provides an overview of the semantic objects defined in the
Matrix, used in the specification of the values of {\sc cont} in signs, both
words and phrases.  Figure~\ref{mrsobj} gives a portion of the type hierarchies
under {\it avm} and {\it sort}, including all of the basic semantic objects.\mpt{Fix wide figure}

\begin{figure}[ht]
\begin{center}
{\it\small
\begin{tree}
\psset{treefit=loose}
\br{*top*}{
\br{avm}{	
 {\br{mrs}{\lf{psoa}\lf{\ \ \ \ \ \ \ \ \ nom-obj}}}
 \lf{\ \ \ lexkeys}
 \lf{\ \ \ \ \ \ \ \ \ \ \ keys}
 {\br{\ \ \ \ \ \ relation}{\tlf{\ldots}}}
 \lf{qeq}
 \lf{\ \ \ \ \ hook}
 \br{semarg}{\br{individual}{\tlf{\ldots}}\lf{\ \ \ \ handle}}
 \lf{tam}
 \lf{\ \ \ png}}
\br{sort}{\lf{\ \ \ \ \ \ \ \ \ semsort}}}
\end{tree}
}
\end{center}
\caption{Type Hierarchy of Basic Semantic Objects}
\label{mrsobj}
\end{figure}

\subsection{\it mrs}
\label{mrssec}

The flat semantic representations assigned to each word
or phrase in MRS comprise three components: 

\begin{enumerate}
\item {\sc rels} - a bag of atomic predications, each
with a `handle' (used to express scope relations) and
one or more roles;
\item {\sc hcons} - a set of handle constraints which
reflect syntactic limitations on possible scope relations
among the atomic predications;
\item {\sc hook} - a group of distinguished externally visible
attributes of the atomic predications in {\sc rels}, used
in combining the semantics of this sign with the semantics
of other signs.
\end{enumerate}

Thus objects of type {\it mrs} (i.e., the value of the features
{\sc cont(ent)} and {\sc c(onstructional)-cont} of {\it sign}s in
the Matrix) are constrained as in (\ref{mrs-cons}).

\es{\label{mrs-cons}
\begin{avm}
{\it mrs}: \[ hook & hook\\
              rels & diff-list\\
              hcons & diff-list \]
\end{avm}
}
%
The value of {\sc rels} is a difference list\footnote{These lists are
implemented as difference list to allow for appending without
relational constraints.} of {\it relation}s (\S\ref{reltypesec}) and
the value of {\sc hcons} is a difference list of {\it qeq}s
(\S\ref{qeqsec}).  The value of {\sc hook} is a feature structure of
type {\it hook} (\S\ref{hooksec}).  Finally, the value of {\sc msg} is
a feature structure of type {\it basic\_message}, used to constrain the
propositional type of a sign (\S\ref{msgsec}).

The type {\it mrs} also has two subtypes {\it nom-obj} and {\it psoa},
following \namecite{Pol:Sag:94}, corresponding to semantic representations of 
nominal signs (with a distinguished {\it ref-ind}) and predicative signs
(with a distinguished {\it event}).  The types {\it ref-ind}
and {\it event} are described in \S\ref{semargsec} below.  This distinction
is encoded in the following two constraints:\footnote{On {\it hook}, 
see \S\ref{hooksec} below.}

\es{\label{psoa}
\begin{avm}
{\it psoa}: \[ hook & \[ index & event \]\]
\end{avm}} \mpt{Are we actually using these for anything?  What work do they do?}

\es{\label{nomobj}
\begin{avm}
{\it nom-obj}: \[ hook & \[ index & index \]\]
\end{avm}}

\subsection{{\it relation}}
\label{reltypesec}

The heart of an {\it mrs} is a bag of elementary predications,
implemented here as a difference list of objects of type {\it
relation} -- i.e., the value of the feature {\sc rels}.\footnote{Note
\label{dlfn} that the Matrix doesn't include typed difference lists,\mpt{Actually, it should.  Create rels-list, and have value of RELS be [ LIST rels-list ].
Likewise for hcons-list, where we add an additional type hcons supertype to qeq to allow other kinds of hcons on an hcons-list.}
so that the value of {\sc rels} is in fact only constrained to be a
difference list without further restriction -- it could just as well be a
difference list of {\it synsem}s.  By convention, and in order to
produce well-formed semantic representations, however, it is always a
difference list of {\it relation}s.} All relations bear values for the
three features introduced on the type {\it relation}, as shown in
(\ref{relation}):

\es{\label{relation}
\begin{avm}
{\it relation}: \[ lbl & handle\\
                   pred & string\\
                   wlink & list \]
\end{avm}
}

\subsubsection{\sc lbl}

The value of {\sc lbl} is a {\it handle}, which is used to express
scope relations.  The {\sc lbl} value of one relation may be identified
with (i) the {\sc lbl} value of one or more other relations, (ii) a
role within some other relation, (iii) a value within a handle
constraint ({\it qeq} -- see \S\ref{qeqsec} below), and/or the value
of {\sc ltop} in a {\it hook} (see \S\ref{hooksec} below).  Relations sharing a
{\sc lbl} value are interpreted as conjoined.  For example, in {\it
The big dog slept.}, the relations introduced by {\it big} and {\it
dog} will share a {\sc lbl} value.  {\it Qeq} constraints and direct
identification of {\sc lbl} values with argument roles in other
relations are used to express scopal interactions among relations.  For
the most part, this is achieved with {\it qeq} constraints, which identify
semantic argument positions where quantifiers can intervene scopally when
underspecified {\it mrs}s are resolved to fully scoped representations.
For those handle argument positions where quantifiers cannot intercede,
direct reentrancy between a {\sc lbl} value and a role is employed.  For an
example, see \S\ref{msgsec} below.  Note that we will often refer to
the {\sc lbl} value of a relation as its handle.

\subsubsection{\sc pred}

The value of {\sc pred} is a string, which serves to distinguish
particular relations.  Earlier versions of the ERG
included a separate type for each distinct relation, leading to a very
flat (and very large) type hierarchy.  We have since found it preferable 
to distinguish
relations via the string-valued {\sc pred} feature, and to reserve the
subtypes of {\it relation} (see \S\ref{relarg}) for those types
that introduce features.  This change also opens the door to more
interesting re-entrancies of {\sc pred} values, explored in the ERG for
the semantics of degree specifiers, and perhaps also useful for some
treatments of coordination or gapping.

For compatibility with RMRS \cite{Cop:03} and software designed 
to integrate deep and shallow processing, {\sc pred} values should
conform to the following templates:

\es{
\begin{tabular}[t]{ll}
{\tt \_orth\_pos\_sense\_rel} & (lexically introduced predicates)\\
{\tt sense\_rel} & (abstract predicates introduced by constructions)
\end{tabular}
}

The {\tt orth} component is a string corresponding to the (stem)
orthography of the lexical entry, at least for all open-class words,
and typically also for closed-class words.  By using the stem orthography,
we enusre that predicate names used in {\it mrs}s produced by deep
grammars will be interoperable with predicate names used in {\it mrs}s 
produced by robust shallow processors, which name the predicates based
on (lemmatized) forms in the input.  The leading underscore is used to
distinguish predicate names introduced by specific lexical entries from those
introduced by constructions or by lexical types supplying a common predicate
for a class of lexical entries.

The {\tt pos} component\mpt{what about a (adjective or adverb) and s
(sahen)?} is one of a closed set of single lowercase letters
interpreted as follows:\\\mpt{Create a second example which reflects
a language with different pos distinctions; emphasize that the point
here is coarse-grained WSD, and not universal pos inventory.}

\es{
\begin{tabular}[t]{ll}
n & noun \\
v & verb \\
a & adjective/adverb \\
p & adposition \\
q & determiner (quantifier) \\
x & all other closed-class predicates
\end{tabular}
}
%
We use this POS-based information (such as might be accessible from a
POS-tagger) for coarse-grained sense distinctions.  Finer-grained
distinctions can be made (in a precision grammar) via the {\tt sense}
component.  The {\tt sense} component can consist of any sequence of
characters (letters, numbers, etc.), excluding the underscore which is
used to separate the components of the name.  In the ERG, verb
particle constructions are handled semantically by having the verb
contribute a relation particular to that combination.\mpt{Illustrate
with example, provide pointer forward.}  We distinguish these
relations by placing the particle's orthography in the {\tt sense}
field. Unlike the other components, the {\tt sense} component is
optional, and if omitted, its separating underscore is also omitted.
By convention, a predicate name with no sense component is interpreted
as underspecified for sense, so if more than one sense is present in
the lexicon for a given orthography and part of speech, each of these
predicate names should have a sense component.

Every relation and predicate name ends in {\bf \_rel}, for the
convenience of the grammar writer, particularly to avoid possible
namespace collisions.  This suffix (and the leading underscore) can of
course be suppressed by MRS display methods if desired.

So for example, the following predicate names are correct for the 
corresponding words:\\ \mpt{Add example strings for each.  Include
\_bank\_n\_1\_rel.}

\es{
\begin{tabular}[t]{ll}
{\it aardvark} & {\bf \_aardvark\_n\_rel}\\
{\it bank}     & {\bf \_bank\_n\_2\_rel}\\
{\it bank}     & {\bf \_bank\_v\_turn\_rel}\\
{\it look}   & {\bf \_look\_v\_up\_rel}\\
\end{tabular}\\
}

Finally, one further detail of formatting should be mentioned:
Words with single lexical entries whose orthography is conventionally
spelled with a space, such as the English use of {\it ad hoc}, appear with the
whole orthography in the {\tt orth} component, but with the space(s)
replaced by the plus sign.  So the following example is also correct:\\

\es{
\begin{tabular}[t]{ll}
{\it ad hoc} & {\bf \_ad+hoc\_j\_rel}\\
\end{tabular}
}

\subsubsection{\sc wlink}

The feature {\sc wlink} serves to link the relation to an element in
the input string, so that applications can reconstruct the input string
source of a given relation in the corresponding semantics.
Since this feature does not interact with the rest of the semantic
composition machinery in the Matrix, we will omit it from our AVM
descriptions in the rest of this paper.  It is included as part of the
Matrix because it provides a useful experimental link later on, when 
developing applications that use a Matrix-derived grammar.  The grammar
engineer can safely ignore this feature during grammar development.  If 
an application seesm to require it, contact the authors for details on
how to put it to use.\mpt{Update to LNK and CFROM/CTO, provide actual
explanation.}

\subsection{{\it qeq}}
\label{qeqsec}

As mentioned above, scopal relations in MRS are represented via
handles, which appear as the value of the feature {\sc lbl} and also
as the value of certain roles within scopal relations (e.g., the {\sc
rstr} value of quantifiers -- see \S\ref{quantsec}).
However, for many applications, including machine translation, fully
specified scope relations are not required.  Furthermore, any surface
string with multiple noun phrases (and therefore multiple quantifiers,
overt or implicit) is going to be ambiguous with respect to scopal
relations.  Rather than build separate parses for each scoping, and
potentially have to choose between them in a given application, it is
preferable to leave scopal relations underspecified, to the extent
that the grammar doesn't in fact constrain them.\fn{For discussion of
underspecification of scope, see \namecite{Cop:Fli:Sag:Pol:03} and
references given there.}  In Matrix-derived grammars, following MRS 
specifications, this is achieved as follows:

\begin{enumerate}
\item The {\sc body} (i.e., scope) of all quantifiers is left unconstrained.
\item Most other handle-taking argument positions are not directly linked 
to the handle of some relation.
\item Rather, the two are related via a {\it qeq} (`equality modulo 
quantifiers') constraint.
\end{enumerate}

\noindent
Thus the {\sc body} value of quantifier relations can be resolved in
such a way that the quantifiers `float in' wherever there's a `space'
left by a {\it qeq} constraint.

In the implementation, these handle constraints (restrictions on scopal
interactions) are represented via the feature {\sc
hcons}.  The value of {\sc hcons} is a difference list (again,
representing a bag) of {\it qeq}s.\fn{Once again, the difference list
is not typed.  See note~\ref{dlfn}.}  {\it Qeq}s, in turn, are
constrained as follows:

\es{
\begin{avm}
{\it qeq}: \[ harg & handle\\
	      larg & handle \]
\end{avm}
}

\noindent
The {\sc harg} is identified with the handle-taking argument position
and the {\sc larg} is identified with the {\sc lbl} (handle) of the outscoped
relation.  Examples of parts of the grammar that impose such constraints
are given in \S\ref{hconssec} below.

While the MRS specification in \namecite{Cop:Fli:Sag:Pol:03} leaves
open the possibility of different kinds of handle constraints, only
{\it qeq} constraints have proven necessary so far for wide-coverage
grammars of English (ERG) and Japanese (JACY), and so only
{\it qeq} constraints are currently implemented in the 
Matrix.\footnote{However, a large grammar implementation for German 
(described in \namecite{Wahl:Karg:00})
makes crucial use of {\it leq} (less than or equal) scopal
constraints, and recent work on extending MRS representations to robust 
processing \cite{Cop:03} also employs {\it leq} constraints.}

\subsection{{\it hook}}
\label{hooksec}

Where the values of {\sc rels} and {\sc hcons} give a rich representation of 
the semantics of any given sign (word or phrase),
more information is needed in order to be able to combine the semantic
representations of signs to build
semantic representations of larger phrases.  For example, consider the
partial feature structures for the {\it mrs}s produced by the ERG for the 
signs {\it the dog} and {\it barks} in
(\ref{db1}):\fn{Following \lkb\ conventions, difference lists are
represented with the brackets $\langle$! !$\rangle$, consistent with the
abbreviatory conventions employed in the TDL \cite{Kri:Sch:94}
syntax adopted in the \lkb.}

\ees{\label{db1}
\item \begin{avm}
\[ {\sc rels} & \q<\tn{!} 
               \[ 
		  lbl & {\it handle}\\
                  pred & {\bf \_the\_q\_rel}\\
                  arg0 & \@2\\
                  rstr & \@3\\
                  body & {\it handle} \],
               \[ lbl & \@5\\
                  pred & {\bf \_dog\_n\_rel}\\
                  arg0 & \@2 \] \tn{!}\q>\\
   hcons & \q<\tn{!} \[ {\it qeq}\\
	           harg & \@3\\
                   larg & \@5 \] \tn{!}\q> \]
\end{avm}
\item \begin{avm}
\[ {\sc rels} & \q<\tn{!} \[ 
                  lbl & {\it handle}\\
                  pred & {\bf \_bark\_v\_rel}\\
                  arg0 & event\\
                  arg1 & semarg \] \tn{!}\q>\\
   hcons & \q<\tn{!} \tn{!}\q> \]
\end{avm}
}
   
In composing the {\it mrs} for the larger phrase {\it the dog barks},
we would like to identify the {\sc arg0} of the {\bf \_dog\_n\_rel} with the
{\sc arg1} of the {\bf \_bark\_v\_rel}.  But how can the relevant parts of
the grammar (in this case, the lexical entry for {\it bark}) gain
access to the right value of the right relation?  While the value of
{\sc rels} is implemented as a difference list, it is notionally to be
treated as a bag, so it would be unprincipled
to make reference to the position of a relation in
the list.  In fact, it would also be impractical for a grammar of any
interesting size: the list position of the
relation contributed by the head noun in a noun phrase
is affected by what else there is in the NP.  For example, in {\it the
big dog}, the {\bf \_big\_j\_rel} would intervene (in this implementation)
between the {\bf \_the\_q\_rel} and the {\bf \_dog\_n\_rel}.  The solution is 
to use the value of the attribute {\sc hook} to `publish' or make visible 
externally just those elements of the {\it mrs} that the grammar will need
access to for semantic composition.

Thus the value of {\sc hook} represents a hypothesis about which 
information may be accessed externally.  The Matrix provides for
three compositionally relevant properties of a sign's semantics, encoded as
features for objects of type {\it hook}:

\es{
\begin{avm}
{\it hook}: \[ ltop & handle\\
               index & individual\\
               xarg & individual \]
\end{avm}
}

\noindent
The value of {\sc ltop} is the local top handle, the handle
of the relation(s) with the widest scope within the constituent, modulo
quantifiers.   This attribute
is accessed by semantic heads in phrasal constructions in order
to impose further scopal constraints involving that handle when composing
the semantics of the phrase.

The value of {\sc index} is the distinguished non-handle variable supplied
by the sign, identified with the {\sc index} of the semantic head daughter,
and usually the {\sc arg0} of the main relation introduced by the syntactic 
head of
the constituent.  If the {\it mrs} is a {\it nom-obj}, this will be a
{\it ref-ind} (referential index).  If the {\it mrs} is a {\it psoa},
this will be an {\it event}.  (See \S\ref{mrssec} above on {\it psoa}
and {\it nom-obj} and \S\ref{semargsec} below on {\it ref-ind} and {\it
event}.)  
This information is accessed by semantic heads
in order to identify indices (including event
indices) with (non-scopal) argument positions in predications: e.g.,
the {\sc arg1} (barker) of the {\bf \_bark\_v\_rel} or the {\sc arg1}
(modified event) of the relation for an intersective adverb like {\it happily}.

Finally, the value of {\sc xarg} (mnemonic for `external argument') is
the index of the single argument in a phrase (in accusative languages,
typically the subject) which can be controlled.  This information will
be accessed by semantic heads in raising and control constructions,
open adjuncts, and constructions like English tag questions.  See
\S\ref{xargsec} for further exemplification.

\subsection{{\it semarg}, {\it tam}, and {\it png}}
\label{semargsec}

The values of {\sc lbl}, {\sc harg}, {\sc larg}, and all role features
(e.g., {\sc arg0} etc., see \S\ref{relarg}) are objects of (subtypes
of) type {\it semarg}.  This type introduces the non-linguistic feature {\sc
instloc}, which is used for skolemization of variables in generation and 
whose value should never be
further constrained by the grammar.  The hierarchy below {\it
semarg} is shown in Figure~\ref{semarghier}.

\begin{figure}[ht]

\begin{center}
\begin{tabular}{ccccccc}
\mc{4}{c}{\node{1}{\it semarg}} & & \\[.5cm]
\node{2}{\it handle} & \mc{4}{c}{\node{3}{\it individual}} & & \\[.5cm]
\mc{2}{r}{\node{4}{\it index}} & \mc{5}{c}{\node{5}{\it event-or-ref-index}}\\[.5cm]
\node{6}{\it expl-ind} & \mc{2}{c}{\node{7}{\it ref-ind}} & \mc{2}{c}{\node{8}{\it conj-index}} & \mc{2}{c}{\node{9}{\it event}}\\[.5cm]
& & \mc{2}{c}{\node{10}{\it conj-ref-ind}} & \mc{2}{l}{\node{11}{\it conj-event}} &\\
\end{tabular}
\end{center}

\ncon{1}{2}
\ncon{1}{3}
\ncon{3}{4}
\ncon{3}{5}
\ncon{4}{6}
\ncon{4}{7}
\ncon{5}{7}
\ncon{5}{8}
\ncon{5}{9}
\ncon{7}{10}
\ncon{8}{10}
\ncon{8}{11}
\ncon{9}{11}

\caption{Type hierarchy rooted in {\it semarg}}
\label{semarghier}
\end{figure}\mpt{Add handle-or-ref-ind (p) type.}

There are two subtypes of {\it semarg}.  The first, {\it handle},
is the type of the value of all 
handle-taking features ({\sc lbl}, {\sc harg}, {\sc larg},
{\sc rstr}, {\sc body}), and can be the value type for a semantic role
feature {\sc arg1, arg2, ...}.  
There are no subtypes of {\it handle}, nor is it
anticipated that grammar engineers will need to add any in the course
of grammar development.  Furthermore, there are no features introduced
on the type {\it handle} (although it inherits {\sc instloc} from
{\it semarg}).

The other immediate subtype of {\it semarg} is {\it individual},
which includes expletive indices ({\it expl-ind}), event indices (or
event variables; {\it event}) and ordinary referential indices ({\it
ref-ind}).  In addition, the Matrix provides for coordinate
indices ({\it conj-ind} with subtypes {\it conj-event} and {\it
conj-ref-ind}) which are introduced by coordination constructions (see
\S\ref{coordsec}).  Note that {\it conj-event} and {\it conj-ref-ind}
inherit from {\it event} and {\it ref-ind}, respectively.  This
ensures that they are compatible with any environment that requires an
{\it event} or {\it ref-ind}, and, conversely, that it is not possible
to specifically select a non-coordinated event or referential
index.\footnote{The hierarchy could of course be extended to allow
such selection, but we expect that it will not be required.}

The contrast between {\it expl-ind} and {\it event-or-ref-index} is
used to constrain the distribution of expletive NPs (e.g., English
{\it it} and {\it there}, German {\it es}, or French {\it
il}).\footnote{Although in general we intend the types given in the
Matrix core grammar to be largely language-independent, 
{\it expl-ind} is probably not useful in languages
which allow pro-drop to the extent that Japanese does, and which therefore
have no use for expletive elements.  Once there is a libary implementing
expletive pronouns, this type will be moved out of the core grammar.}  Such expletive pronouns are given
an {\sc index} value of type {\it expl-ind} (which is exceptionally not
linked to any argument position in their main relation, since they
don't introduce any relation).  Ordinary selecting heads can then require 
dependents with
contentful {\sc index} values (i.e., {\it event-or-ref-index}s, or
some subtype thereof).  In languages like English (and perhaps Dutch) with
multiple expletive pronouns, subtypes of {\it expl-ind} can be added
to distinguish the different pronouns.  In the ERG, we have found
this technique useful not only for implementing the selection of 
particular expletive pronouns by particular heads (e.g., {\it rain} vs.\
existential {\it be}), but also for pronoun matching in tag questions
(see \citeboth{Ben:Fli:99}).

The types {\it event} and {\it ref-ind} each introduce one feature,
as shown in (\ref{evref}):

\ees{\label{evref}
\item \begin{avm}
{\it event}: \[ tam & tam \]
\end{avm}

\item \begin{avm}
{\it ref-ind}: \[ png & png \]
\end{avm}
}
%
The features {\sc tam} and {\sc png} encode tense-aspect-mood
information (properties of events) and agreement information (person,
number, and gender -- properties of referential indices; cf.\
\citeboth{Pol:Sag:94}), respectively.  In the current version of the
Matrix, the type {\it tam} (unsurprisingly) introduces the
features {\sc tense}, {\sc aspect}, and {\sc mood}.  This may be too
strong a constraint, since languages may well conflate two or more of
these properties; see the discussion of the {\it png} type immediately
following.  We will have more to say about the analyses of agreement in
\S\ref{AgrTAM} below.

\es{
\begin{avm}
{\it tam}: \[ tense & tense\\
              aspect & aspect\\
              mood & mood \]
\end{avm}
}
%
The type {\it png}, on the other hand, introduces no features, though 
it is intended to provide the locus for constraints on (semantic) person, 
number, and gender.  These three dimensions are not necessarily distinct
for a given language, as seen in English where person and number are
usually conflated morphologically, motivating a basic distinction between
third-singular and non-third-singular inflectional types, with further
subtypes for non-third-singular.\footnote{Cf. \namecite{Flickinger00} for
discussion of this part of the type hierarchy.} Thus in English the most 
natural feature structure for the {\it png} type is to have a merged 
person-number {\sc pn} attribute along with a gender {\sc gen} attribute.
Clearly, in other languages separate attributes for person and number are
well motivated, leading us to leave the elaboration of the {\it png} type
as language-specific for now.

\subsection{{\it semsort}}
\label{semsort}

The type {\it semsort} is the root of a hierarchy of sorts which
serves to represent grammaticized semantic distinctions which are used
in the selection of dependents (subjects, complements, specifiers, and
modifiers).  We expect it to include a small hierarchy of
grammatically salient semantic types such as {\it animate} or {\it
time}, as well as more specific types to support selection of
closed-class items, such as preposition selection by verbs or
auxiliary matching in English tag questions \cite{Ben:Fli:99}.  Note
that {\it semsort} is made a subtype of {\it sort} to make explicit
the claim that these types used for semantic selection do not
themselves introduce attributes of their own.\mpt{This section needs to
be fixed: SORT and semsort should be axed, the rest of it used to illustrate
KEYS/KEY and predsort.  Combine with next section.}
% Dan - and why do we predict that?

The ERG illustrates several uses of semantic selection, which motivate
some particular subtypes of {\it semsort}.  These sorts in the ERG
include at least the human/nonhuman distinction reflected in the
choice of relative pronoun ({\it the person *which/whom I met}), and
the various semantic subtypes introduced by prepositions (temporal,
locative, directional, stative, etc.).  Verb-preposition dependencies
in English, for example, can be encoded by having the verb constrain
the {\sc key} value of its PP complement.  In some cases, the
selecting verb might not be looking for a particular preposition's
{\sc key} value, but rather impose a more abstract constraint on a
complement's key, such as the PP complement for the English verb {\it
put}, whose {\sc key} value might be constrained to those introduced
by a subclass of locative prepositions, to preclude analyzing
e.g. {\it *Kim put the chair for Sandy.}  Thus both the leaves and
the intermediate types in the {\it semsort} hierarchy can be useful.

The subhierarchy under {\it semsort} will be language-specific,
reflecting grammaticized semantic properties motivated by
constructions in that language.  The analysis of Japanese numeral\mpt{Look into Francis's work and either elaborate this point, or axe.}
classifiers, for example, can be implemented using these semantic
sorts, as can the quite idiosyncratic constraints for English on
preposition selection with temporal nouns.  Here, for example, noun
phrases like {\it Tuesday} or {\it the fifteenth} (denoting days of
the week or days of the month) introduce the semantic sort {\it day}
which is selected by (the relevant lexical entry for) the preposition
{\it on} but not {\it in}, to admit {\it Kim arrived on Tuesday} but
not {\it *Kim arrived in Tueaday}.  Such idiosyncratic collocational
constraints often reflect natural semantic distinctions, but are not
predictable cross-linguistically.

% Maybe move this somewhere else? --EB
Note that, as strings, {\sc pred} values are not organized into a hierarchy.
Furthermore we expect to only need to identify particular relations
(via {\it semsort}s which correspond to specific {\sc pred} values)
in a reasonably small number of closed classes of lexical items.
We therefore see no value in providing an external pointer to the
{\sc pred} value of the main predication contributed by the lexical
head (syntactic or semantic) of a phrase.




\subsection{{\it keys}}
\label{keysec}

The next type of object to consider is {\it keys}, not exactly a semantic
object, but one useful for the interface between syntax and semantics.  
This type is
introduced to serve as the value of the (syntactic) {\sc head} feature 
{\sc keys}. 
The purpose of {\sc keys} is to provide constraints for semantic
selection of dependents (complements, specifiers, subjects, and
modifiers).  The attributes in {\it keys} include two called {\sc key}
and {\sc altkey}, which can be used by selecting heads or by constructions
to constrain a dependent phrase semantically.
Since the {\sc keys} attribute is a {\sc head} feature,
the grammar ensures that these semantic properties of a phrase
propagate up the syntactic head path.

The type {\it keys} is constrained as shown in (\ref{keys}):

\es{\label{keys}
\begin{avm}
\[ key & predsort \\
   altkey & predsort \]
\end{avm}
}

The feature {\sc key} provides a constraint on the {\it predsort} of a phrase,
which can be but is not necessarily identified with the {\sc pred} value of one
of the relations in the {\sc rels} list of that phrase.  Since generalizations
about semantic selection may make reference to more than one such semantic
property of a sign, the Matrix provides a second {\sc keys} attribute called
{\sc altkey}.  The idea here is that a modifier, for example, may constrain the
phrase it modifies along one dimension of semantic selection, while a verb
taking that same modified phrase as a complement may need to constrain it along
a second dimension.

Note that as a head feature, the value of {\sc keys} on the mother node of a
headed-phrase will be identified with the {\sc keys} value of the head 
daughter.  For non-headed phrases such as coordinate constructions, each
such construction type will have to stipulate the values for the attributes
in {\sc keys} on the mother node.  These values may come from one or the other
of the daughters, or may be supplied directly by the construction itself.


\subsection{{\it lexkeys}}
\label{lexkeysec}

The last type of object to consider here is the type {\it lexkeys}, which
is defined in the Matrix to provide attributes which are not linguisticslly
significant, but which provide some convenient shorthand notation for the
grammar writer when defining the lexical type hierarchy.  In particular,
this type introduces two attributes that simplify the expression of
constraints on relations introduced by a lexical type, and two attributes
that point to the {\sc key} attribute of complements of the lexical type..
The grammar writer can decide whether or not to make use of these shorthand
attributes (the latter two bearing a leading double dash as a reminder that 
they are only abbreviations for longer path names).  
The type {\it lexkeys} is the value of the {\sc local}  feature {\sc lkeys},
and nothing in the Matrix propagates the value of this feature up
to phrases.  Thus, the features on {\it lexkeys} are only
used (if at all) in simplifying the definitions of lexical entries.\mpt{Actually, we now think that KEYREL and ALTKEYREL do do interesting linguistic work.
In particular, they facilitate the factorization of lexical types into
different dimensions, including a linking dimension which need not know
anything about the actual RELS list.  Separate ``not linguistically
interesting'' from ``not projected into the syntax''.}

The type {\it lexkeys} is constrained as follows:

\es{
\begin{avm}
{\it lexkeys}: \[ keyrel & relation\\
    altkeyrel & relation\\
    {-}{-}compkey & semsort\\
    {-}{-}ocompkey & semsort \]
\end{avm}
}

\noindent

The two features {\sc keyrel} and {\sc altkeyrel} are available to provide
pointers to each of two relations introduced by a lexical entry, for easier
definitions of constraints on values of those relations within the lexical
type hierarchy.  Typically, the value of {\sc keyrel} will be a pointer
to the relation in the {\sc rels} list of a lexical entry which introduces
its {\sc index} value as the {\sc arg0} of that relation.  The value of
{\sc altkeyrel} will be unbound for most lexical entries, since
most introduce a single relation in the {\sc rels} list, but for
an entry which introduces more than one relation, this attribute provides
a convenient pointer to a second relation on the {\sc rels} list.

The features {\sc {-}{-}compkey} and {\sc {-}{-}ocompkey} provide
pointers to the {\sc key} values of two complements of a lexical
entry.  The relationship
between {\sc {-}{-}compkey}/{\sc {-}{-}ocompkey} and {\sc keys.key} of
the relevant complements will be established in lexical types.\fn{In the
case of the entry for {\it abstain} given here, the relevant constraint
in the ERG is on the type {\it unsat\_two\_arg\_subst}, which is a supertype of
the {\sc synsem} value of {\it v\_empty\_prep\_intrans\_le}.}
This allows specific lexical entries to do semantic selection of
complements by simply constraining their own {\sc {-}{-}compkey} and/or
{\sc {-}{-}ocompkey} values, as in the following (partial) lexical entries 
adapted from
the ERG for the verb {\it abstain} and the empty preposition {\it from} as in 
{\it Kim abstained from the vote}:\mpt{Update: Now treating selected prepositions as truly semantically empty.}

\es{
\begin{avm}
{\it abstain\_v1}: \[ \avmspan{\it v\_empty\_prep\_intrans\_le}\\
		     stem & \q< \textnormal{``abstain''} \q>\\
                     synsem & \[ local \[ lkeys \[ keyrel & {\bf \_abstain\_v\_from\_rel}\\
{-}{-}compkey & {\bf \_from\_p\_sel\_rel} \]\]\]\]
\end{avm}
}

\es{
\begin{avm}
{\it from\_prtcl}: \[ \avmspan{\it p\_prtcl\_le}\\
		     stem & \q< \textnormal{``from''} \q>\\
                     synsem & \[ local \[ lkeys \[ keyrel \[ pred & {\bf \_from\_p\_sel\_rel} \]\]\]\]\]
\end{avm}
}

The lexical type for the verb {\it abstain} includes as part of its definition
the following reentrancy which enables the use of this {\sc {-}{-}compkey} 
shortcut:

\es{
\begin{avm} \[ \avmspan{\it v\_empty\_prep\_intrans\_le}\\
synsem & \[ loc \[ cat \[ val \[ comps \[ first \[ loc \[ cat \[ head \[ keys \[ key & \@1 \]\]\]\]\]\]\]\]\]\\
lkeys \[ {-}{-}compkey & \@1 \]\]\]
\end{avm}
}

The lexical type for semantically empty prepositions includes the following
reentrancy which shows how the constraints in the two lexical entries above
will interact correctly, without making any reference to the attribute
{\sc lkeys} during processing, as desired.

\es{
\begin{avm} \[ \avmspan{\it p\_prtcl\_le}\\
synsem & \[ loc \[ cat \[ head \[ keys \[ key & \@1 \]\]\]\]\\
lkeys \[ keyrel \[ pred & \@1 \]\]\]\]
\end{avm}
}

\subsection{Summary}

This concludes our tour of the basic semantic objects defined in the
Matrix.   In this section we have briefly touched on some aspects
of the linguistic analyses that motivate each type of object.  The
following sections flesh out these linguistic analyses in more detail.

\section{Relations and Argument Features}
\label{relarg}

This section describes the various subtypes of {\it relation} posited
in the Matrix and provides guidelines for introducing new subtypes of
{\it relation}.  The top of the type hierarchy below {\it relation}
is shown in Figure~\ref{relhier}.

\begin{figure}[ht]
\begin{center}
{\it
\begin{tree}
\br{relation}{\br{\hspace{2.0in}arg0-relation}{\br{\hspace{-0.9in} noun-relation}{\lf{\hspace{-0.9in}named-relation}}
	   	             \lf{\hspace{0.5in} event-relation}
		             \lf{\hspace{1.9in} arg1-relation}
		             \lf{\hspace{3.2in} adv-relation}
			     \lf{\hspace{4.6in} quant-relation}}
		             \br{subord-or-conj-reln}{}}
\end{tree}
}
\end{center}
\caption{Partial type hierarchy below {\it relation}}
\label{relhier}
\end{figure}

In the following subsections we will describe
ordinary predicates (i.e., the subtypes of {\it arg0-relation} introduced by
nouns, verbs, prepositions, adjectives and
adverbs -- \S\ref{ordpred}), and the special relation types defined
for quantifiers (\S\ref{quantsec}), messages (\S\ref{msgrelsec}), and
subordination and coordination (\S\ref{coordsec}).

While we expect that grammar development for particular languages will
require the addition of some abstract relation types to the set
proposed here, we recommend not positing a type for each lexical
relation, but rather using the feature {\sc pred} (see
\S\ref{reltypesec} above) to distinguish different lexical relations
of the same type.  New relation types are merited only in two circumstances:
(i) when a feature needs to be introduced that is relevant for some relations
but not others; or (ii) when the values of one or more attributes of an 
existing relation are consistently constrained in the same way across multiple
contexts in the grammar.

\subsection{Ordinary Predicates}
\label{ordpred}

\subsubsection{{\it arg0-relation} subtypes}
\label{arg0sec}

The vast majority of lexical entries introduce only a single relation,
and furthermore one that is an instance of {\it arg0-relation} or one of its
subtypes, including {\it noun-relation}, {\it event-relation}, and {\it adv-relation}.  
The type {\it arg0-relation} is constrained as follows:

\es{
\begin{avm}
{\it arg0-relation}: \[ arg0 & individual \]
\end{avm}
}

Thus all open-class lexical items introduce a relation with the {\sc
arg0} role.  The value of this role is an {\it individual}.  For nouns,
verbs, and adjectives, the {\sc arg0} of the main relation (i.e., the 
{\sc keyrel} value) should be identified with
the {\sc hook.index}, and this identification should be done on a general
supertype.\mpt{This is now done in matrix.tdl, and that should
be described.}

For ordinary nouns, the value of {\sc arg0} will be a (referential) index 
which serves as a pointer
to the entity referred to by the NP.  This
same index is also the value of the {\sc arg1} feature in any relations
introduced by intersective modifiers of the head noun, and also the value of 
a role feature in the relation of any lexical predicate selecting the NP as a 
semantic argument.  The type
{\it noun-relation} is constrained appropriately:

\es{
\begin{avm}
{\it noun-relation}: \[ arg0 & ref-ind \]
\end{avm}
}
%
As discussed in \S\ref{semargsec} above, objects of type {\it ref-ind}
bear the feature {\sc png}.  This means that the agreement information
associated with a noun will also be found in {\sc lkeys.keyrel.arg0},
but this information doesn't propagate up to the phrase, and so
(semantic) agreement information should always be accessed through the
{\sc cont.hook.index}.\footnote{The Matrix also provides a syntactic
{\sc agr} feature, to handle cases where syntactic agreement properties
do not align with the semantic representations.  See \citeboth{Ben:Fli:99}.}

For verbs, which introduce relations of type {\it event-relation}, the
value of {\sc arg0} is the event variable identified as the {\sc
index} value.  Again, this {\it event} will also show up as the value
of a role feature in modifier relations such as those expressed by
(intersective) adverbs and by PPs modifying verbal phrases.  Other
elements that might appear to notionally take events as arguments
(e.g., clause-embedding verbs or scopal adverbs like {\it probably})
actually take the handle of the phrase as their argument.  The type
{\it event-relation} is constrained as follows:

\es{
\begin{avm}
{\it event-relation}: \[ arg0 & event \]
\end{avm}
}
%
As discussed in \S\ref{semargsec} above, objects of type {\it event}
bear the feature {\sc tam}.  This means that tense, aspect and mood
information associated with an event are encoded in the {\sc arg0}
of the relation describing that event.  Again, as there is no pointer
to the whole relation that gets passed up to the phrasal level,
this information and
the event index itself should be accessed via the path {\sc cont.hook.index} 
for semantic selection or composition. 

The Matrix provides one particularly useful subtype of {\it noun-relation} 
called {\it named-relation}:

\es{
\begin{avm}
{\it named-relation}: \[ pred & {\bf named\_rel}\\
                         carg & string \]
\end{avm}
}
%
This type is used for proper names (including names of months, days of\mpt{Move to quantifier-less representation of proper names and pronouns.}
the week and days of the month), and it introduces a feature {\sc
carg} (`constant argument') which takes as its value a string
representing the name of the named entity.  It further constraints the
{\sc pred} value to be {\bf named\_rel}.  Thus all proper nouns
contribute relations of the same type and with the same pred value.
Their contribution to the semantics is distinguished solely by their
{\sc carg} (and of course their {\sc arg0}s, which will be distinct for
each proper name in a single sentence).

Note that as {\it named-relation} is a subtype of {\it noun-relation}, it
will introduce a referential index ({\it ref-ind}) which must
be bound by some quantifier in order to form a well-formed {\it mrs}.
In the ERG, this is achieved by means of a non-branching rule
which adds the quantifier to a proper noun, which usually but not always
lacks an explicit determiner.  Given noun phrases in English like {\it the
younger Smith} and {\it some Roberts}, the ERG defines lexical entries
for proper names as syntactically nouns, not NPs, and employs a specialized
unary syntactic rule to construct a noun phrase from an N-bar headed by
a proper noun (and usually consisting only of that noun).  The Matrix will
support this analysis of proper names, but will of course also enable the
construction of grammars for languages where proper names are simply lexical 
NPs, in which case each lexical entry's semantics will consist of two
relations: a {\it named-relation} and a {\it quant-relation} to bind its 
referential
index.

\subsubsection{{\it arg1-relation}, {\it arg12-relation}, ...}

As discussed in \S\ref{arg0sec}, the value of {\sc arg0} is the
distinguished {\it ref-ind} or {\it event} of a relation.  Many
relations, of course, take further arguments.  These are represented
with the features {\sc arg1}, {\sc arg2}, {\sc arg3} and {\sc arg4}.
It is important to note that there is no independent interpretation
of thematic roles attached to these feature names.  That is, {\sc arg1} 
cannot be taken as equivalent to something like {\sc agent} wherever 
it is used.
Rather, we understand the precise interpretation of the role names to
be dependent on the relation they appear in.  This interpretation is
to be specified in a separate component of the grammar called the
Sem-I (`semantic interface'), which provides such information as is
needed to map from {\it mrs}s to application-specific representations.
The Sem-I is further described in \S\ref{semisec} below.

Figure~\ref{arg1234fig} shows the type hierarchy below {\it arg0-relation},
filling in the information below {\it event-relation}, {\it noun-relation}, 
and {\it arg1-relation},
which was abbreviated in Figure~\ref{relhier} above.  The features
{\sc arg1} through {\sc arg4} are introduced by the types {\it
arg1-relation} through {\it arg1234-relation}, as shown in (\ref{argavm}).
The values of the argument features on these types are only
contrained to be {\it semarg}s, as for any given predicate
they may be {\it individual}s ({\it ref-ind}s or {\it event}s), or
{\it handle}s in the case of scopal predicates.

\begin{figure}[ht]
\begin{center}

\begin{tabular}{cccccc}
\mc{6}{c}{\node{1}{\it arg0-rln}}\\[.5cm]

\node{2}{\it adv-rln} & \node{4}{\it arg1-rln} & \node{5}{\it event-rln} &
\node{6}{\it noun-rln} & \node{7}{\it quant-rln} &\\[.5cm]

\node{8}{\it verb-ellipsis-rln} & 
\node{9}{\it arg12-rln} & \node{10}{\it arg1-ev-rln} & \node{3}{\it noun-arg1-rln} & \node{11}{\it named-rln} & \\[.5cm]

\node{12}{\it unspec-compound-rln} &
\node{14}{\it arg123-rln} & \node{15}{\it arg12-ev-rln} & &\\[.5cm]

\node{13}{\it prep-mod-rln} & \node{16}{\it arg1234-rln} & \node{17}{\it arg123-ev-rln} & & &\\[.5cm]

&& \node{18}{\it arg1234-ev-rln} & & &
\end{tabular}

\ncon{1}{2}
\ncon{1}{4}
\ncon{1}{5}
\ncon{1}{6}
\ncon{1}{7}
\ncon{4}{8}
\ncon{4}{9}
\ncon{4}{10}
\ncon{4}{3}
\ncon{5}{10}
\ncon{6}{3}
\ncon{6}{11}
\ncon{9}{12}
\ncon{9}{14}
\ncon{9}{15}
\ncon{10}{15}
\ncon{15}{13}
\ncon{14}{16}
\ncon{14}{17}
\ncon{15}{17}
\ncon{16}{18}
\ncon{17}{18}

\end{center}
\caption{Hierarchy below {\it arg0-relation}}
\label{arg1234fig}
\end{figure}\mpt{Remove adv-rel}

\ees{\label{argavm}
\item \begin{avm}
{\it arg1-relation}: \[ arg1 & semarg \]
\end{avm}
\item \begin{avm}
{\it arg12-relation}: \[ arg2 & semarg \]
\end{avm}
\item \begin{avm}
{\it arg123-relation}: \[ arg3 & semarg \]
\end{avm}
\item \begin{avm}
{\it arg1234-relation}: \[ arg4 & semarg \]
\end{avm}
}

Note that, in keeping with the strategy for interpretation outlined
above, it is not possible to have an {\sc arg3} without an {\sc arg1}
and an {\sc arg2}.  That is, if a relation takes two arguments 
beyond its distinguished event/index ({\sc arg0}), they will always
be labelled {\sc arg1} and {\sc arg2}.  
Furthermore, the {\sc arg1}
should always correspond to the first (least oblique) syntactic
argument, the {\sc arg2} to the second (next least oblique) syntactic
argument, and so on (see \S\ref{linksec}).\mpt{Oblique isn't the right
notion here --- it has to do with `deep subject', etc.  Fix and point
to new description of linking types.}

The type {\it arg1-relation} does not inherit from {\it
event-relation}, to allow for the possibility of relational nouns and
other relations that take more than one argument but do not express
events.  The types {\it arg1-ev-relation} through {\it
arg1234-ev-relation} provide event relations with one to four
arguments.  Thus (semantically) intransitive verbs introduce {\it
arg1-ev-relation}s, (semantically) transitive verbs introduce {\it
arg12-ev-relation}s, etc.  Ordinary nouns introduce {\it
noun-relation}s, which only have an {\it arg0}.  Relational nouns
(such as {\it picture}) introduce a relation {\it noun-arg1-relation}
which inherits from both {\it noun-relation} and {\it arg1-relation}
(not {\it arg1-ev-relation}).  A noun with two semantic arguments
would require a relation that inherits from {\it noun-relation} and
{\it arg12-relation}, and similarly for a noun with three arguments,
if such exist (apart from deverbal nouns,\mpt{What is the current
analysis of deverbal nouns?  This sounds like something to include in
the phenomena section.}  which are treated for English via lexical
rule in the ERG).  These types all inherit from {\it arg0-relation}
and through it {\it relation}, so they will all also bear the features
{\sc arg0}, {\sc lbl}, {\sc pred}, and {\sc wlink}.

Note that verbs are not the only lexical entries which introduce
subtypes of {\it event-relation} in their semantics.  Adjectives can
in some languages serve directly as predicates, be subject to tense
and aspect constraints, and take one or more arguments, all analogous
to verbs.  Likewise, prepositions can also appear as the heads of
predicative phrases, so they will also introduce subtypes of {\it
event-relation} in their semantics.  It is worth noting that the
choice of the name {\it event-relation} is intended to include states
as well as processes, as required by verbs as well as adjectives and
prepositions.  At present the Matrix does not introduce events into
the relations for nouns, though it might be argued that examples in
English like {\it the current president} motivate treating at least
some nouns as event-bearing.  Since we do not know of grammar-internal
constraints that require making reference to events on nouns, we will
expect such temporally constrained noun phrases to be interpreted
outside of the grammar.

Like verbs, adjectives and prepositions identify their {\sc
hook.index} value with the {\sc arg0} value in their main relation,
enabling them to appear as heads of predicative phrases, supplying an
event which can be constrained for tense and aspect.  In the ERG, for
example, the copula {\it be} does not supply its own relation, but
only constrains tense and aspect since it often combines with a verb
participle that already supplies the event relation, as in {\it Kim
was leaving}.  Since that\mpt{This paragraph is just too dense.
Extend in chapter on specific analyses, provide pointer forward and
highlights here.}  grammar uses the same copula {\it be} for {\it Kim
was angry}, the adjective {\it angry} must supply an event relation
whose {\sc arg0} event will be constrained in this example to past
tense.  Now since adjectives and prepositions can head phrases that
modify nouns, they must also expose the argument position they will
identify with the index of the phrase they modify.  This ``external
argument'' of a PP or AP is its {\sc hook.xarg} value, identified in
the lexical head's feature structure with the value of the rather long
path {\sc head.mod.first.local.cont.hook.index}.  This value will be
unified by the grammar rules for modification with the semantic index
of the noun being modified.  These two {\sc hook} attributes {\sc
index} and {\sc xarg} enable the semantic composition desired for
intersective modification of nouns by adjectives or PPs, and of verbs
by PPs, as well as the use of adjectives and prepositions in
predicative constructions.  The machinery should extend to more
interesting examples like {\it The dog currently in the park is
barking}, where {\it currently} temporally constrains the event
introduced by the PP {\it in the park} (its {\sc hook.index}) while
the PP's {\sc hook.xarg} is identified with the {\sc hook.index} of
the noun {\it dog}.

Adverbs,\mpt{out of date} unlike adjectives and prepositions, do not appear predicatively, and
hence do not need to introduce an independent event.  So they simply identify
the value of their main relation's {\sc arg0} with the index of the phrase
they modify, and further identify this value with their own {\sc hook.index}.

\subsection{Special Relations}

The Matrix also provides a small number of additional semantic relations which
introduce additional attributes for easier readability for both grammar
writers and application developers.  These special relations could of course
be defined just using the above inventory of relation attributes ({\sc pred}
and {\sc arg1 ... argn}), since the Sem-I would provide an unambigous
interpretation of these features within a given relation type.  But it has
proven to be convenient in practice to supply the following relations with
their own particular attributes.

\subsubsection{Quantifiers}
\label{quantsec}

The lexical types for quantifiers like {\it some} and {\it every} introduce
a {\it quant-relation} which is also a subtype of {\it arg0-relation}, but
thie type introduces two additional features, {\sc rstr} (restrictor)
and {\sc body}.
As scopal features, they both take values of type {\it handle}.  The
{\sc rstr} will be related to the top handle of the quantifier's
restriction (i.e., the N$'$ that takes the quantifier as a specifier)
via a {\it qeq} constraint (see \S\ref{qqeq} below for details on
how this constraint is introduced).  The {\sc body} is left unbound:
this is what allows quantifiers to have varied scoping possibilities
(see \citeboth{Cop:Fli:Sag:Pol:03}).  The value of {\sc arg0} is the
referential index that the quantifier binds (see \S\ref{bvsec}).

The type {\it quant-relation} is thus constrained as follows:

\es{
\begin{avm}
{\it quant-relation}: \[ arg0 & ref-ind\\
                    rstr & handle\\
                    body & handle \]
\end{avm}
}

Since all quantifiers should need only these features, particular
quantifiers should be distinguished by their {\sc pred} values
(e.g., {\bf \_every\_q\_rel}, {\bf \_some\_q\_rel}, {\bf \_the\_q\_rel} in
the ERG).
The Matrix does not yet adopt any hypothesis about whether the inventory
of quantifier relations might be made language-independent, though it
would clearly be desirable if such an inventory could be defined.\mpt{Elaborate? Ref to papers on different kinds of quantifiers, relationship between
our quantifier rels and GQs as the semantic side of NPs.}


\subsubsection{Subordination and Coordination}
\label{coordsec}

The Matrix provides two relation types for subordination and coordination,
both subtypes of {\it subord-or-conj-relation}, which introduces two new
attributes for their semantic arguments.
The first subtype {\it subord-relation} can be used for
subordinating conjunctions like English {\it since}, and the second
subtype {\it conjunction-relation} is used by ordinary conjunctions like 
English 
{\it and}.  Both coordination and subordination relations take two handles as 
arguments; in the
case of subordination, these correspond to the {\sc ltop} values of
the main and subordinate clauses, while for coordination they correspond to
the {\sc ltop} values of the two conjuncts.  These
are the values of the features {\sc l-hndl} and {\sc r-hndl}, with the names
chosen to encode explicitly the order of left and right conjuncts,
since this order can be semantically (or pragmatically) relevant, at least for
coordination.\mpt{Out of date: subord now just arg12}

\es{
\begin{avm}
{\it subord-or-conj-relation}: \[ l-hndl & handle\\
                                  r-hndl & handle \]
\end{avm}
}

The main relation introduced by specific subordinating conjunctions (e.g., 
{\it if}, {\it because}, {\it while}) will be simply the general relation
{\it subord-relation}, with each lexical entry providing distinguishing 
values for the {\sc pred} attribute.  There
is, however, more to be said about coordination.  Coordination of clauses,
predicates or noun phrases requires the construction of a single {\it
individual} ({\it event} or {\it ref-ind}) that can be the value of a
role feature in any outside predicate that takes the conjoined
entity as an argument.  This is achieved by using {\sc arg0} for
the conjoined index and giving {\it conjunction-relation}
the two additional features shown in (\ref{conjrel}):\mpt{Detail and refs on collective v. distributive readings?}

\es{\label{conjrel}
\begin{avm}
\[ arg0 & conj-index\\
   l-index & index\\ 
   r-index & index \]
\end{avm}
}
%
The value of {\sc arg0} is the conjoined index ({\it conj-index}) which
serves as a pointer to the separate conjoined entity.  

Of course, it is possible to conjoin more than two noun phrases,
predicates or clauses.  In order to keep a determinate number of
features for any given relation, we represent multi-coordination via a
series of chained binary {\it conjunction-relation}s, 
where the {\sc r-index} of
all but the lowest {\it conjunction-relation} is the {\sc c-arg} of the next
{\it conjunction-relation}.\mpt{Give example here (or maybe in specific
analyses chapter with pointer forward here).}

Note that in the coordination of noun phrases, the {\sc l-hndl} and 
{\sc r-hndl} values of the {\it conjunction-relation} will be identified 
with the
{\sc ltop} values of each of the two noun phrases, but these handle values
will not be identified with the {\sc lbl} values of any relations in the
phrase, given our treatment of quantifiers as underspecified with respect to
scope.  In all other cases of coordination (clauses, VPs, predicative phrases,
etc.), the values of {\sc l-hndl} and {\sc r-hndl} will be identified with
the {\sc lbl} values of relations in the phrase.\mpt{See what Rui has to say
about the right answer to this.}


\subsubsection{Miscellaneous Special Relations}

In order to illustrate some ways in which this hierarchy of semantic relations
might be extended, the Matrix provides three additional relations which 
grammar writers may find useful.  Perhaps the most interesting of these is
the {\it unspec-compound-relation}, intended for use in the construction of
noun-noun compounds.  These are treated in the ERG as a syntactic constituent 
with two noun daughters that are combined by a rule which introduces this
{\it unspec-compound-relation}, a subtype of {\it arg12-relation}.  This
relation does not introduce any new attributes, but rather constrains the
two arguments to both be of type {\it ref-ind}.  Such a constraint might have
value, but if not, the grammar writer could simply use the relation type
{\it arg12-relation}, and assign the appropriate unique {\sc pred} value.\mpt{No need for these types. Introduce instead as distinguished {\sc pred} values
in specific analyses section.}

Similarly, the Matrix supplies the {\it verb-ellipsis-relation} as a subtype of
{\it arg1-relation}, intended for analyses of elided verb phrases as in the
English example {\it We tried to but we couldn't.}  Here neither new attributes
nor specific constraints on existing attributes are added, though we 
anticipate that the semantics of elliptical constructions in languages might
require such additional machinery.  Again, the grammar writer may well choose
to ignore this type.

The third such type is the {\it prep-mod-relation}, a subtype of 
{\it arg12-ev-relation}, and intended for use with prepositions that head
modifier phrases.  Here again, no new constraints enrich this type, though
for a given language it may be useful to impose additional type constraints
on the values of {\sc arg1} and {\sc arg2} that would hold for all lexical
entries and constructions introducing this relation type.  

\subsubsection{User-Defined Special Relations}

The Matrix design anticipates that grammar writers may choose to extend the
inventory of relation types to accommodate language-specific phenomena.  For
example, an analysis of degree specifiers and measure phrases like the 
English examples {\it the building was very tall} or {\it the building was 
fifty meters tall} may lead the grammar writer to a new relation type for
these degree phrases.  Similarly, comparative constructions, like in the 
English examples {\it Kim is taller than Sandy was} or {\it dogs have more
legs than ostriches}, may require the introduction of relations with 
additional attributes to encode all of the relevant constraints.\mpt{More detail
on these in specific analyses section.}

When adding a new relation, be sure to place it correctly in the {\it relation}
hierarchy so that it inherits exactly the attributes and constraints that it
needs from existing types.  And before choosing to make the addition, consider
whether an existing relation type might in fact serve, given the existence of
the Sem-I component (described in \S\ref{semisec} below) to provide a unique
interpretation of the attributes {\sc arg1 ... arg4} for each relation.\mpt{Odd use of second person reference}

\section{Features of Indices: Agreement and TAM}
\label{AgrTAM}

The Matrix currently provides only minimal support for more fine-grained
constraints on semantic indices, in part due to the high degree of variation
needed across grammars to encode constraints on attributes like person, number,
and gender for referential indices, or tense, aspect, and mood for events.
Even a seemingly uncontroversial step like introducing separate attributes for 
each of these properties on indices has proven to be unproductive, since
a given language may conflate two such attributes morphologically and
syntactically.\mpt{Add COG-ST and SF.}

For example, the ERG captures the distribution of person and number in English
by introducing types which conflate these two properties, enabling a direct
expression of the relevant syntactic contrasts without requiring a disjunctive
representation of the constraints on subject-verb agreement.  The ERG 
introduces an attribute of the type {\it png} called {\sc pn} (person-number),
whose values are of the type {\it pernum} defined as in Figure~\ref{pernumfig}.
Given these types, the constraint on the {\sc agr} value for
non-third-singular verbs is simply\\

\es{
\begin{avm}
\[ {\it non3sg-verb} \\
   {\sc agr} & \[ {\sc pn} & {\it non3sg} \] \]
\end{avm}\\
}

\noindent
since the type {\it non3sg} will unify with any of its subtypes, including the
{\sc agr} values {\it 1sg, 2sg, 1pl, 2pl}, and {\it 3pl}.


\begin{figure}[ht]
\begin{center}
{\setlength{\tabcolsep}{1mm}
\begin{tabular}[t]{ccccc}
\multicolumn{5}{c}{\node{pernum}{\it pernum}}\\[.2in]
\multicolumn{2}{c}{\node{13sg}{\it 1or3sg}} & \multicolumn{3}{c}{\node{non3}{\it non3sg}}\\[.2in]
\node{3sg}{\it 3sg} & \node{1sg}{\it 1sg} & \multicolumn{3}{c}{\node{non1}{\it non1sg}}\\[.2in]
& \multicolumn{2}{c}{\node{2per}{\it 2per}} & \node{1pl}{\it 1pl} & \node{3pl}{\it 3pl}\\[.2in]
& \node{2sg}{\it 2sg} & \node{2pl}{\it 2pl} &&\\[.2in]
\end{tabular}
}
\nodeconnect{pernum}{13sg}
\nodeconnect{pernum}{non3}
\nodeconnect{13sg}{3sg} 
\nodeconnect{13sg}{1sg} 
\nodeconnect{non3}{1sg} 
\nodeconnect{non3}{non1} 
\nodeconnect{non1}{2per}
\nodeconnect{non1}{1pl}
\nodeconnect{non1}{3pl}
\nodeconnect{2per}{2sg}
\nodeconnect{2per}{2pl}
\end{center}

\caption{Hierarchy of {\sc pn} values in the ERG}
\label{pernumfig}
\end{figure}

This generalization would be difficult to express without disjunctions if
person and number were distinct attributes, and we expect similar conflations
in particular languages for other agreement attributes, so we have left the
internal structure of the values for the attributes {\sc png} and {\sc tam}
unspecified in the Matrix.  In future work, we hope to include possible
tense/aspect systems as modules in the Matrix, i.e., as components that we 
expect to be useful in many but not all languages.\mpt{Update with pointers
to Laurie and Scott's work.}

The treatment of tense, aspect, and mood is equally underspecified in the
current version of the Matrix, though it is expected that a more elaborated
candidate set of types will emerge in the near future, at least for some
aspectual distinctions, growing out of work on the NorSource grammar of
Norwegian.

\section{Syntax-Semantics Interface}
\label{synsem-i}

Section \ref{basicobj} discussed the basic semantic objects defined
in the Matrix.  Section \ref{relarg} discussed types of relations
and the representation of arguments of relations.  The preceding
section sketched our approach to representing agreement and TAM.
This should give a fairly complete picture of the kinds of semantic
representations Matrix-derived grammars should build.  This section
addresses how to go about building those representations
compositionally as words are combined into successively larger phrases
in the syntax.  This section is organized as follows:
\S\ref{semprinsec} describes the implementation of general semantic
principles. \S\ref{semheadsec} describes the identification of
semantic heads.  \S\ref{ccontsec} discusses constructions and lexical
rules that make semantic contributions beyond simply combining the
semantics of their daughters.  \S\ref{linksec} describes the linking
of syntactic and semantic arguments within the lexicon, and the
role of syntactic rules in associating the indices of syntactic
arguments with the appropriate semantic requirements of a predicate.
\S\ref{bvsec} describes how to ensure that all nominal indices are
bound by an appropriate quantifier.  \S\ref{hconssec} provides
examples of handle constraints, including constraints on the {\sc
rstr} values of quantifiers, how messages interact with handle
constraints, and handle constraints concerning scopal modifiers like
{\it probably}.  
%\S\ref{semselkey} describes how the value of {\sc
%keys} can be used to implement semantic selectional restrictions.
%[Now covered in {lexkeysec}.)
Finally, \S\ref{xargsec} describes mismatches between syntax and
semantics, including control constructions, expletives, and
raising of {\sc key} and {\sc index} values (e.g., by the English auxiliary 
{\it do}).

\subsection{Semantic Principles}
\label{semprinsec}

With every word or phrase providing a semantics which consists of {\sc
hook}, {\sc rels}, and {\sc hcons}, the principles of semantic
composition in phrase structure rules can be stated (and implemented)
quite elegantly, following the definitions in \namecite{Cop:Las:Fli:01}:

\begin{enumerate}
\item The value for {\sc rels} on the mother of a phrase is the result of
appending the {\sc rels} values of all of its daughters.
\item The value for {\sc hcons} on the mother of a phrase is the result of
appending the {\sc hcons} values of all of its daughters.
\item The value for {\sc hook} on the mother of phrase is identified with
the {\sc hook} value of its semantic head daughter, where each phrase type
uniquely determines which of the daughters is the semantic head.
\end{enumerate}

In the Matrix, principles 1 and 2 are
implemented as constraints on a few high-level types ({\it lex-rule},
{\it basic-unary-phrase} and {\it basic-binary-phrase}) within the
{\it sign} subhierarchy (sketched in Figure~\ref{signhier}), such that they
are inherited by all phrases and lexical rules.  In addition, the type
{\it phrase-or-lexrule} identifies the mother's {\sc hook} with the
{\sc hook} of the semantics provided by the rule itself (the value of
a feature called {\sc c-cont}; see \S\ref{ccontsec} below).  More
specialized phrase types identify the {\sc hook} of the {\sc c-cont}
with the {\sc hook} of the head or non-head daughter, as appropriate.\mpt{Add ternary phrases! Explain purpose of these types.}

\begin{figure}[ht]
\begin{center}
%\begin{small}
\hspace{-20pt}\begin{tabular}[t]{ccccccc}
\mc{7}{c}{\node{1}{\it sign}}\\[.5cm]
\mc{3}{c}{\node{2}{\it word-or-lexrule}} & \mc{4}{c}{\node{3}{\it phrase-or-lexrule}}\\[.5cm]
\node{4}{\it word} & &\node{5}{\it lex-rule} & & \node{6}{\it phrase}\\[.5cm]
&&\node{7}{\ldots} & \node{8}{\it basic-} & \node{9}{\it basic-} & \node{10}{\ldots}\\
&&&{\it unary-} & {\it binary-}\\
&&&{\it phrase} & {\it phrase}\\
\end{tabular}
\nodeconnect{1}{2}
\nodeconnect{1}{3}
\nodeconnect{2}{4}
\nodeconnect{2}{5}
\nodeconnect{3}{5}
\nodeconnect{3}{6}
\nodetriangle{5}{7}
\nodeconnect{6}{8}
\nodeconnect{6}{9}
\nodeconnect{6}{10}
%\end{small}
\end{center}
\caption{Subhierarchy under {\it sign}}
\label{signhier}
\end{figure}

Principles 1 and 2 require the monotonic accumulation of {\sc rels} and {\sc
hcons} values from daughter to mother in a phrase.  The values of
these features are implemented as difference lists (typed feature
structures which bear values for two attributes {\sc list} and {\sc
last}), which allows us to state the accumulation of values using the
same single operation of unification of typed feature structures.
(\ref{dla}) shows the constraints on the type {\it
basic-unary-phrase}, including the `diff-list appends' that implement
principles 1 and 2.\mpt{Explain or point to explanation of C-CONT here. 
Somewhere mention Fregean compositionality `and how they are put together'.}

\enumsentence{\label{dla}
\begin{avm}
{\it basic-unary-phrase}: \[ \avmspan{synsem.local.cont \[ rels & \[ list & \@1\\
        		            last & \@2 \]\\
			  hcons & \[ list & \@4 \\
				     last & \@5 \]\]}\\
   c-cont & \[ rels & \[ list & \@3\\
		         last & \@2 \]\\
	      hcons & \[ list & \@6\\
		         last & \@5 \]\]\\
   \avmspan{args \< \[ \ldots \[ cont \[ rels & \[ list & \@1\\
						   last & \@3 \]\\
					 hcons & \[ list & \@4 \\
						     last & \@6 \]\]\]\] \>} \]
\end{avm}
}

\subsection{{\sc hook} Features and Semantic Heads}
\label{semheadsec}

The type {\it head-compositional} (a subtype of {\it headed-phrase})
provides the constraint that identifies the {\sc hook} of the {\sc
c-cont} (and therefore also of the mother) with the {\sc hook} of the
head-daughter:\mpt{Add intro: There are three possibilities.}

\es{
\begin{avm}
{\it head-compositional}: \[ c-cont.hook & \@1\\
                             head-dtr & \[ synsem.local.cont.hook & \@1 \]\]
\end{avm}
}
%
The types {\it basic-head-comp-phrase},
{\it basic-extracted-comp-phrase}, and {\it
basic-head-opt-comp-phrase} (and therefore its two subtypes)
all inherit this constraint from {\it head-compositional}.  That is,
these types of phrases are all cases where the syntactic head and the
semantic head are the same.   Note that {\it basic-extracted-comp-phrase} 
and {\it basic-head-opt-comp-phrase} are unary phrases, but it is still
necessary to determine whether or not they are head-compositional, as
there are also unary phrases which contribute constructional semantics
and therefore do not identify the {\sc hook} of the {\sc c-cont} with the
{\sc hook} of the daughter (see \S\ref{ccontsec} below).\mpt{More explanation of what these types do, for non-Matrix users.}

The type {\it head-mod-phrase} does not inherit from {\it
head-compositional}, but one of its subtypes {\it isect-mod-phrase} does, to
account for the semantic properties of phrases with intersective modifiers.
In contrast, one of its other subtypes {\it scopal-mod-phrase}
imposes the opposite constraint, identifying the {\sc hook} values of the 
{\sc c-cont} and the non-head daughter, as required for the semantics of
phrases containing scopal modifiers like {\it probably}.
Thus we treat the modifier in scopal head-modifier constructions as the 
semantic head, even though it is not the syntactic head.  Similarly, in {\it
basic-head-spec-phrase} the {\sc hook} of the {\sc c-cont} is
identified with the {\sc hook} of the specifier daughter, as we take
specifiers to be semantic heads.

%For some reason this constraint is
%actually implemented via the {\sc spr} feature of the head-daughter,
%which seems unnecessarily baroque to me.  Should we fiques it?
%It looks like {\it basic-head-filler-phrase} doesn't
%identify any semantic head.  This probably needs to be fixed, too. [DPF] 
%Agreed.  Probably want a type nonhead-compositional for spr-hd and head-adj.

The third possibility is illustrated in the type {\it head-subj-phrase},
which supplies a message relation that takes widest scope, and hence the
{\sc c-cont.hook} of the phrase is not identified with that of either daughter.
Instead, the {\sc c-cont.hook.ltop} value is identified with the handle (label)
of the newly supplied message relation, and the {\sc c-cont.hook.index} value
is identified with that of the head daughter.  See the next section for a
discussion of how constructions can contribute semantics.\mpt{Need a new example.}

So we have (syntactically) headed phrases where the semantic head is
the syntactic head, headed phrases where the semantic head is instead
the (syntactic) non-head daughter, and headed phrases where the {\sc
c-cont} is the semantic head.  The Matrix does not provide any
subtypes of {\it non-headed-phrase}, but every phrase must have
some semantic head, even if only the {\sc c-cont}.  The constraints
on {\it lex-rule}, {\it basic-binary-phrase}, and {\it basic-unary-phrase}
ensure that the {\sc hook} of the mother will always be identified
with the {\sc hook} of the {\sc c-cont}, but it is the responsibility
of the grammar developer to make sure that the {\sc hook} of the
{\sc c-cont} provides sufficient information.  This will be ensured if
it is identified with the {\sc hook} of a daughter, or if the {\sc c-cont} has
a non-empty {\sc rels} value and the features inside {\sc hook}
are related to the appropriate parts of some {\it relation} on 
{\sc rels}.

The next subsection provides some examples of semantically
contentful constructions.

\subsection{Semantic Contributions of Constructions}
\label{ccontsec}

Since some phrase types may introduce semantic content which is not
drawn from any of the daughters of the phrase, the MRS framework
provides an attribute for phrasal signs called {\sc c-cont} (for
construction content), which behaves with respect to the semantics
principles just as though it were another daughter of the phrase (see,
for example, (\ref{dla}) above).  {\sc c-cont} is implemented in the
Matrix as a top-level attribute of phrases and lexical rules,
introduced on the type {\it phrase-or-lexrule}.  Like {\sc cont}, its
value is of type {\it mrs}.  

If a phrase does not introduce any additional semantic content of its
own, the values for the attributes {\sc rels} and {\sc hcons} in {\sc
c-cont} will be empty lists, so unary and binary phrases can safely
always append these values to those supplied by the syntactic
daughters.  Likewise, the {\sc hook} value of a phrase is always
identified with its {\sc c-cont}'s value for {\sc hook}, where for
most phrases this {\sc hook} in {\sc c-cont} will simply be identified
with that of one of the daughters of the phrase, namely the semantic
head daughter.\mpt{Why are we doing this?  What is gained over just
having head-compositional identify mother's HOOK with head-dtr's, etc.}

\subsubsection{Syntactic Constructions}

We illustrate construction-introduced semantic content with the
treatment of noun-noun compounds in the ERG.  In the analysis of the
sentence {\it the office chair arrived}, the phrase {\it office chair}
is built using a syntactic rule specifically for noun-noun compounds,
and this rule introduces a generic two-place relation {\bf
n-n-cmpnd\_rel} which relates the variables introduced by the two
nouns.  The syntactic structure is sketched in (\ref{compound}), where
the head-specifier rule is used to combine the determiner and the
compound noun, while the head-subject rule combines the full NP with
the verb phrase {\it arrived}.  The corresponding MRS semantics is
shown in (\ref{compsem}):

\enumsentence{\label{compound}
\begin{tree}\psset{treefit=tight}
\br{S}{\br{NP}{\br{Det}{{\lf{the}}}
               \br{N}{\br{N}{\lf{office}}
                      \br{N}{\lf{chair}}}}
       \br{VP}{\br{V}{\lf{\hspace{0.2in}arrived}}}}
\end{tree}\\
\\
{\it The office chair arrived}
}

\enumsentence{\label{compsem}
\begin{avm}
\[ {\it mrs}\\
hook & \[ ltop & h1\\
          index & e2 \]\\
rels & \avml\q<\tn{!} \[ \avmspan{\bf proposition\_m\_rel}\\
              lbl & h1\\ 
              marg & h4 \],
            \[ \avmspan{\bf \_the\_q\_rel}\\
               lbl & h10\\
               arg0 & x8\\
               rstr & h11\\
               body & h12 \],
            \[ \avmspan{\bf \_chair\_n\_rel}\\
               lbl & h7\\
               arg0 & x8 \],
            \[ \avmspan{\bf \_office\_n\_rel}\\
               lbl & h14\\
               arg0 & x16 \], \\
            \[ \avmspan{\bf udef\_q\_rel}\\
               lbl & h17\\
               arg0 & x16\\
               rstr & h18\\
               body & h19 \],
            \[ \avmspan{\bf n-n-cmpnd\_rel}\\
               lbl & h7\\
               arg1 & x8\\
               arg2 & x16 \],
            \[ \avmspan{\bf \_arrive\_v\_rel}\\
               lbl & h21\\ 
               arg0 & e2\\
               arg1 & x8 \] \tn{!}\q>\avmr\\
hcons & \q<\tn{!} \[ {\it qeq}\\ 
                     HARG & h4\\ 
                     LARG & h21 \],
                  \[ {\it qeq}\\
                     HARG & h11\\
                     LARG & h7 \],
                  \[ {\it qeq}\\
                     HARG & h18\\
                     LARG & h14 \]\ \tn{!}\q> \]
\end{avm}

%% $\avmplus{\att{mrs}\\
%%           \attval{HOOK}{\avmplus{\attval{LTOP}{h1}\\
%%                                  \attval{INDEX}{e2}}}\\
%%           \attvallist{RELS}{
%% \avmplus{\att{prpstn}\\
%%          \attvaltyp{LBL}{h1}\\
%%          \attvaltyp{MARG}{h4}},
%% \avmplus{\att{def}\\
%%          \attvaltyp{LBL}{h10}\\
%%          \attvaltyp{ARG0}{x8}\\
%%          \attvaltyp{RSTR}{h11}\\
%%          \attvaltyp{BODY}{h12}},
%% \avmplus{\att{chair}\\
%%          \attvaltyp{LBL}{h7}\\
%%          \attvaltyp{ARG0}{x8}},
%% \avmplus{\att{office}\\
%%          \attvaltyp{LBL}{h14}\\
%%          \attvaltyp{ARG0}{x16}},
%% \avmplus{\att{udef}\\
%%          \attvaltyp{LBL}{h17}\\
%%          \attvaltyp{ARG0}{x16}\\
%%          \attvaltyp{RSTR}{h18}\\
%%          \attvaltyp{BODY}{h19}},\\
%% \avmplus{\att{n-n-cmpnd}\\
%%          \attvaltyp{LBL}{h7}\\
%%          \attvaltyp{ARG1}{x16}\\
%%          \attvaltyp{ARG2}{x8}},
%% \avmplus{\att{arrive}\\
%%          \attvaltyp{LBL}{h21}\\
%%          \attvaltyp{ARG0}{e2}\\
%%          \attvaltyp{ARG1}{x8}}}\\
%% \attvallist{HCONS}{
%%     \avmplus{\att{qeq}\\
%%              \attval{HARG}{h4}\\
%%              \attval{LARG}{h21}},
%%     \avmplus{\att{qeq}\\
%%              \attval{HARG}{h11}\\
%%              \attval{LARG}{h7}},
%%     \avmplus{\att{qeq}\\
%%              \attval{HARG}{h18}\\
%%              \attval{LARG}{h14}}}}$
}

This MRS representation contains two noun relations each associated with
a quantifier relation to bind the variables that are their {\sc arg0} values.
The two noun variables are also identified with the 
{\sc arg1} and {\sc arg2} attributes of the {\bf n-n-cmpnd\_rel} relation, and
the variable for the head noun {\it chair} is also the value of the single
argument of the {\it arrive} relation.  The {\it n-n-cmpnd} relation is 
introduced by the grammar in the {\sc rels} attribute of the {\sc c-cont} 
of the grammar rule for noun-noun compounds, which also identifies the 
assignments of the two nominal instance variables (supplied by its two
daughters) to the {\sc arg0} and {\sc arg1} attributes of that {\it n-n-cmpnd}
relation.  The relevant constraint on the grammar rule is sketched in
(\ref{nnc}):

\enumsentence{\label{nnc}
\scriptsize
\begin{avm}
  \[ head-dtr...hook & \[ index \@1 \]\\
    non-head-dtr...hook & \[ index \@2 \]\\
    \avmspan{c-cont \[ rels.list \< \[ \avmspan{\it n-n-cmpnd}\\
				  arg1 & \@1\\
                                  arg2 & \@2 \] \> \]}\]
\end{avm}
}

As discussed above, general principles of semantic composition that
are encoded in the Matrix types ensure that rule-specific relations
are gathered up along with the relations supplied by the daughters of
the rule, and that the appropriate external semantic hooks (the {\sc
ltop} and {\sc index} values) are identified on the phrase itself,
ready for further composition.


\subsubsection{Lexical Rules}

Lexical rules are treated in the Matrix as a particular type of unary
rule, in most respects like syntactic unary rules, though lexical
rules are prevented from interleaving with syntactic rules.  Thus
semantic composition for lexical rules is implemented using the same
principles as outlined above for syntactic phrases.  A lexical rule
may or may not introduce semantic content of its own; if it does, that
content is found in the {\sc c-cont} attribute of the rule and is
combined with the semantic content of the (single) daughter (the
`input' to the lexical rule) by those same principles.

Note that this approach imposes a strong constraint on the
directionality of lexical rules in the Matrix, since the principles of
composition guarantee monotonic accumulation of atomic predications,
so no semantic content from a daughter in a phrase or lexical rule is
ever lost.  For example, a lexical rule relating the
causative/inchoative alternation in English for verbs like {\it break}
will have to treat the inchoative lexical entry as the `input' to the
lexical rule (that is, the daughter), and the semantically richer
causative lexical entry as the `output' (the {\sc synsem} value of the
lexical rule).


\subsection{Linking}
\label{linksec}

Lexical entries can select syntactic arguments, such as complements, subjects,
and specifiers, constraining the properties of these arguments to capture the
relevant subcategorization requirements.  At the same time, these lexical
entries can impose constraints which determine the way that the semantics of 
their arguments will combine with the semantics of the selecting entry.
These constraints linking the semantic hooks of syntactic arguments to the
appropriate semantic argument positions are introduced in lexical entries
and interact with corresponding constraints in the constructions
that combine a word or phrase with one or more of its syntactic arguments.\mpt{Update this whole section with pointers to Matrix types.}

For example, a transitive verb like English {\it chase} subcategorizes for an 
NP subject and an NP object, where in the ERG the verb combines with its 
object using the {\it head-complement} rule, and with its subject using the
{\it head-subject} rule.  The semantic index of the subject NP is assigned
to the {\sc arg1} role in the {\bf \_chase\_v\_rel} relation, while the index of
the object NP is assigned to the {\sc arg2} role, where the interpretation
of these role names (as the chaser and the thing chased, respectively) is
provided by a separate component of the grammar called the Sem-I, described
in \S\ref{semisec}.

The linking type for transitive verbs like {\it chase} in the ERG includes the 
following simple linking constraints:\fn{These \label{argsfn}constraints are 
not stated directly
on the type {\it trans-lt}, but are rather inherited from its supertypes.  We
display them on {\it trans-lt} for expository convenience.  In future versions 
of the Matrix,
we expect to state such constraints using the feature {\sc ARG-S} (Argument Structure) rather than valence features like SUBJ or COMPS.}

\es{
\begin{avm}
\[ {\it trans-lt}\\
   subj & \q< \[ hook.index & \@1 \] \q>\\
   comps & \q< \[ hook.index & \@2 \] \q>\\
   keyrel  & \[ arg1 & \@1 \\
                arg2 & \@2 \] \]
\end{avm}
}

%% $\avmplus{\att{\it transitive-verb}\\
%%           \attvallist{SUBJ}{
%%           \avmplus{\attval{HOOK.INDEX}{\ind{1}}}}\\
%%           \attvallist{COMPS}{
%%           \avmplus{\attval{HOOK.INDEX}{\ind{2}}}}\\
%%           \attvallist{CONT.RELS}{
%%           \avmplus{\attval{ARG1}{\ind{1}}
%%                   \attval{ARG2}{\ind{2}}}}}$\\

When the verb phrase {\it chased the cat} is constructed using the
{\it head-complement} rule, the feature structure for the sign {\it the cat}
is unified with the constraints in the {\sc comps} attribute of {\it chase},
including both the syntactic requirements for an accusative NP and the
semantic constraints which identify the semantic index of that NP with the
{\sc arg2} role in the head's relation {\bf \_chase\_v\_rel}.  An analogous
identification is made when the subject NP {\it the dog} is combined with
the verb phrase {\it chased the cat} using the {\it subject-head} rule, so
that the NP's semantic index is unified with the {\sc arg1} role of the
{\bf \_chase\_v\_rel}.

Lexical entries like the English verb {\it insist} which take sentential
complements differ from simple transitive verbs in that they impose a 
linking constraint on the local top {\sc ltop} attribute of their complement 
rather than on the {\sc index} of that complement.  This ensures that the
semantics of the embedded sentence falls within the scope of the semantic
relation introduced by the selecting verb, as discussed in \S\ref{msgsec} 
below.

The lexical type for verbs like {\it insist} as in {\it Kim insisted that
Sandy was right} in the ERG includes the following linking constraints:\fn{See note \ref{argsfn}.}

\es{
\begin{avm}
\[ {\it cp-intrans-verb}\\
   subj & \q< \[ hook.index & \@1 \] \q>\\
   comps & \q< \[ hook.ltop & \@2 \] \q>\\
   keyrel & \[ arg1 & \@1\\
               arg2 & \@2 \] \]
\end{avm}
}

%% $\avmplus{\att{\it s-comp-verb}\\
%%           \attvallist{SUBJ}{
%%           \avmplus{\attval{HOOK.INDEX}{\ind{1}}}}\\
%%           \attvallist{COMPS}{
%%           \avmplus{\attval{HOOK.LTOP}{\ind{2}}}}\\
%%           \attvallist{CONT.RELS}{
%%           \avmplus{\attval{ARG1}{\ind{1}}
%%                    \attval{ARG2}{\ind{2}}}}}$\\

Lexical entries for modifiers introduce similar linking constraints
on the words or phrases that they modify, even though modifiers are not the
syntactic heads in head-modifier constructions.  These constraints on both
the syntax and semantics of modified phrases are introduced in the 
{\sc head.mod} attribute of a modifier's lexical entry (instead of the
valence features including {\sc subj} or {\sc comps}), and the constructions
that combine modifiers and heads unify the feature structure of the head
with the constraints in the {\sc head.mod} attribute of the modifier, analogous
to the effects of the valence-combining constructions.

For example, the lexical type for simple intersective adjectives like English
{\it tall} includes the following linking constraints:

\es{
\begin{avm}
\[ {\it adj-synsem}\\
   head.mod & \q< \[ hook.index & \@1 \] \q>\\
   keyrel & \[ arg1 & \@1 \]\]
\end{avm}
}

%% $\avmplus{\att{\it intersect-adj}\\
%%           \attvallist{HEAD.MOD}{
%%           \avmplus{\attval{HOOK.INDEX}{\ind{1}}}}\\
%%           \attvallist{CONT.RELS}{
%%           \avmplus{\attval{ARG1}{\ind{1}}}}}$\\

\noindent
Then the {\it intersective-modifier-head} rule which combines {\it tall} with 
{\it chair} unifies the {\sc head.mod} constraints of {\it tall} with the 
feature structure for {\it chair}, and in addition the rule identifies the
{\sc ltop} values of the two daughters, ensuring that the noun and its
modifying adjective are assigned the same scope since they have a common
handle.

Scopal modifiers like the English adverb {\it probably} impose a linking
constraint on the phrase they modify which is analogous to that introduced
by verbs like {\it insist}, making reference to the modified phrase's 
{\sc ltop} attribute rather than its {\sc index}:

\es{
\begin{avm}
\[ {\it basic\_scopal\_adverb\_synsem}\\
  head.mod & \q< \[ hook.ltop & \@1 \] \q>\\ 
  keyrel & \[ arg1 & \@1 \] \]
\end{avm}
}

The construction that combines scopal modifiers with their heads will unify
this constraint from the modifier on the head's {\sc ltop}, resulting in the
semantics of the modified phrase falling within the scope of the relation
introduced by the modifier, as desired.

\subsection{Indices Bound by Quantifiers}
\label{bvsec}

One of the requirements for an {\it mrs} to be well-formed is that each of
the referential indices ({\it ref-ind}) must be bound by a quantifier.\mpt{Rephrase?}  In
the Matrix this means that within a sentence each time a noun phrase is
constructed which introduces a referential index (the {\sc arg0} value of
the relation supplied by the head noun), that {\sc arg0} value 
must be identified with the {\sc arg0} value of exactly one 
{\it quant-relation} in the {\it mrs} for the sentence.  In addition, if the
value of an argument role in some relation {\sc arg1 ... arg4} is constrained\mpt{Reconsider?  Check in current Matrix.}
to be of type {\it ref-ind}, then that variable will have to be bound by a
quantifier.  This means that if the grammar writer defines a lexical entry
for a verb with optional arguments, the values of the {\sc argn} roles in
that verb's relation should be left underspecified when defining the lexical
entry for the verb.  For example, the English verb {\it eat} might be defined
as a transitive verb whose NP object is optional, allowing both {\it The mouse
ate} and {\it The mouse ate the cheese}.  The value of the {\sc arg2} role
in the {\bf arg12-ev-relation} for this optional direct object can be
constrained to be of type {\it index}, but not {\it ref-ind}, in the definition
of the lexical entry for {\it eat}.  If the direct object is not present, the
{\sc arg2} value will then be treated as unbound, and if the direct object is
present, it will supply both the referential index and the quantifier that
binds it.

Note that semantic variables of type {\it event} in the Matrix are not bound by
quantifiers, so they do not affect the well-formedness of an {\it mrs}, even
if they appear as the argument of some predicate.  And indices introduced by
expletive pronouns, which will be (subtypes of) {\it expl-ind}, are not
expected to appear as arguments of relations in an {\it mrs}.\mpt{Find a way to enforce this?}

\subsection{Imposing Handle Constraints}
\label{hconssec}

Since the elementary predications for a phrase or sentence are simply 
collected up as a bag or unordered list in the attribute {\sc cont.rels},
any scope relations among these predications must be expressed by means of
constraints on their handles, the value of the attribute {\sc lbl} in each
elementary predication.  In this subsection we discuss the three most typical
kinds of handle constraints used in Matrix grammars.

\subsubsection{Embedded Clauses}
\label{msgsec}

A clause is taken in the Matrix to be a saturated projection of a verb (or\mpt{Update this section}
perhaps some other tense-bearing predicate) which introduces a message
relation (a proposition, question, or command).  As discussed above, each
message relation introduces just one argument role called {\sc marg}, whose
value will constrain the handle with the next-highest scope in that clause
(ignoring quantifiers), by means of a {\it qeq} constraint 
(cf.\ \S\ref{qeqsec}). 
This next-highest scoping handle will typically be the value of the {\sc lbl}
attribute of the head verb's relation.  For simplex sentences with only one
clause, the {\sc lbl} value of the one message relation will be identified
with the {\sc ltop} of the semantics for the whole sentence, expressing the
fact that this message has scope over every other relation within the
sentence, including all quantifiers.

When one clause is embedded as an argument or modifier phrase within a larger
sentence, the handle of its message relation is instead identified either with 
an argument role of the embedding predicate, or with the {\sc ltop} of the
phrase it is modifying.  This expresses the fact that the semantic content
of this embedded clause falls within the scope of some other relation in the
sentence.  We can illustrate with an example of each of these types of
embedded, first for sentential complements and then for modifier phrases.

In the English sentence {\it Kim wondered whether Sandy arrived}, the clause
{\it whether Sandy arrived} is a complement selected by the verb {\it wonder},
which introduces an {\bf arg12-ev-relation} whose {\sc arg1} value is the
referential index supplied by {\it Kim} and whose {\sc arg2} value is the
{\sc ltop} of the embedded clause.  This {\sc ltop} for {\it whether Sandy
arrived} is identified with the {\sc lbl} of the message relation (of type
{\it question}) as a result of the semantic composition of that clause.

Relative clauses are taken to be intersective modifiers of nouns, and as we
have already seen, intersective modification is expressed in the Matrix by
identifying the {\sc ltop} values of the modifier and the phrase being
modified.  Thus in the sentence {\it Every dog that the cat chased barks}, the
{\sc ltop} of the relative clause {\it that the cat chased} is identified
with the {\sc ltop} of the noun {\it dog}, expressing the fact that the
semantic content of the relative clause will fall inside the scope of the
quantifier {\it every} binding the referential index supplied by {\it dog}.


\subsubsection{Quantifiers and Scope}
\label{qqeq}

As seen above, each referential index introduced in an {\it mrs} must be
bound by a quantifier, but in addition, the handle of the noun relation 
introducing that {\it ref-ind} must also be appropriately constrained.  Each
nominal phrase that is ready to combine with a determiner will identify its
{\sc ltop} with the handle of the highest-scoping elementary predication in
that phrase, typically the {\sc lbl} of the head noun.  When this phrase
becomes a noun phrase, either by combining with a determiner or via some
construction which supplies the quantifier, this {\sc ltop} value of the 
nominal phrase might be expected to be identified with the {\sc rstr} 
(restrictor) value of its quantifier.  But in order to allow for the full
range of possible scope relations within noun phrases, a {\it qeq} handle
constraint is introduced, where the {\sc rstr} value of the quantifier
relation is identified with the {\sc harg} (the ``higher'' scope position)
of the {\it qeq}, and the {\sc ltop} of the N-bar is identified with the
{\sc larg} (the ``lower'' scope position).  This seemingly cumbersome
introduction of a {\it qeq} for the semantics within every noun phrase
ensures that all possible readings of the phrase are correctly represented,
allowing just the right range of variation in quantifier scope.
For a fuller discussion of these issues, see \namecite{Cop:Fli:Sag:Pol:03}.\mpt{Add ex}

\subsubsection{Scopal Modifiers}

Some modifiers like the English adverb {\it probably} are treated as
scopal, which means that instead of taking as their argument the {\it
ref-ind} or {\it event} of the phrase they modify, they identify their
semantic relation's argument position with the top handle of the
phrase they modify.  This expresses the fact that the semantic content
of the modified phrase falls within the scope of the semantic
predicate introduced by the modifier.  As with the other examples of
scopal interactions we have seen, here too we want to allow for
intervening quantifiers in the underspecified representation for a
sentence like {\it Every manager will probably hire some consultants}.
So the lexical entry for {\it probably} does not identify the {\sc
ltop} of the verb phrase it modifies directly with its {\sc
arg0},\mpt{Give ex} but rather introduces a {\it qeq} relation whose
{\sc harg} is identified with the adverb's {\sc arg0} and whose {\sc
larg} is identified with the {\sc ltop} of its {\sc mod}
value.\mpt{maybe mention that these are less common than intersective
modifiers?}
 
\subsection{Syntax-Semantics Mismatches}
\label{xargsec}

When defining the linking between syntactic arguments of a lexical entry\mpt{Update this section with actual Matrix linking types}
and the corresponding semantic argument positions in the relation introduced
by that entry, a grammar writer will often encounter mismatches.  A single
syntactic argument may have its index be linked to
%more than one position in
%the head's semantics 
a position in more than one head's semantics
(as in equi constructions), or it may not appear in
the head's semantic relation at all, as in raising or expletive constructions.
In this section we describe the mechanisms provided in the Matrix for 
defining these mismatches in the syntax-semantics interface.

\subsubsection{External arguments and control}

We have made reference so far to two of the {\it hook} attributes,
{\sc ltop} and {\sc index}, both of which play a crucial role in the semantic
construction of every phrase.  A third attribute, {\sc xarg}, is relevant for
control phenomena such as equi and raising, since it identifies the semantic
index of a phrase's external argument (usually the subject of a verb phrase).
Identifying this property of a phrase as part of the hook allows our general
principles of semantic composition to make this attribute visible for control
of subject-unsaturated complements (VPs, predicative PPs, etc.) and also for
agreement even at the sentence level, as for example in tag questions in
English \cite{Ben:Fli:99}.  An example using this {\sc xarg}
attribute in composition is given in (\ref{control}), with the lexical type
for subject-equi verbs given in (\ref{seq}).

\enumsentence{\label{control}
The dog tried to bark.
}

\begin{tree}
\psset{levelsep=.7in}
\br{S}{
       \br{\begin{avm}\[index & \@1\]\end{avm}}{
              \br{Det}{\lf{the}}
              \br{\begin{avm}\[index & \@1\\ arg0 & \@1 \]\end{avm}}{\lf{dog}}}
       \br{\begin{avm}\[subj & \q< \[ index & \@1 \] \q>\]\end{avm}}{
              \br{V}{\lf{tried}}
              \br{\begin{avm}\[xarg & \@1 \]\end{avm}}{
                     \br{C}{\lf{to}}
                     \br{\begin{avm}\[xarg & \@1\\ arg1 & \@1 \]\end{avm}}{\lf{bark}}}}}
\end{tree}

%% \br{S}{\br{$\avmplus{\attval{INDEX}{\ind{1}}}$}
%%            {\br{Det}{\lf{the}}
%%             \br{$\avmplus{\attval{INDEX}{\ind{1}}\\
%%                           \attval{ARG0}{\ind{1}}}$}{\lf{dog}}}
%%        \br{$\avmplus{\attval{S.INDEX}{\ind{1}}}$}
%%            {\br{V}{\lf{tried}}
%%             \br{$\avmplus{\attval{XARG}{\ind{1}}}$}
%%                 {\br{C}{\lf{to}}
%%                  \br{$\avmplus{\attval{XARG}{\ind{1}}\\
%%                                \attval{ARG1}{\ind{1}}}$}{\lf{bark}}}}}


\enumsentence{\label{seq}
Type for subject-equi verbs like {\it try}

\begin{avm}
\[ {\it subj-equi-verb}\\
   subj & \q< \[ hook.index & \@1 \] \q>\\
   comps & \q< \[ hook.xarg & \@1 \] \q>\\
   keyrel & \[ arg1 & \@1 \] \]
\end{avm}

}

%% $\avmplus{\att{\it subj-equi-verb}\\
%%           \attvallist{SUBJ}{
%%           \avmplus{\attval{HOOK.INDEX}{\ind{1}}}}\\

%%           \attvallist{COMPS}{
%%           \avmplus{\attval{HOOK.XARG}{\ind{1}}}}\\
%%           \attvallist{CONT.RELS}{
%%           \avmplus{\attval{ARG1}{\ind{1}}}}}$


Here the lexical entry for the verb {\it try} identifies its VP complement's
semantic external argument ({\sc xarg} value) with its subject's semantic index
({\sc index} value), and further identifies that index with the appropriate
role (the {\sc arg1}) in the lexical relation introduced by the verb.  The MRS 
semantics constructed for this example is given in (\ref{csem}).


\enumsentence{\label{csem}
\begin{avm}
\[ {\it mrs}\\
   hook & \[ ltop & h1\\
             index & e2 \]\\
   rels & \avml\q<\tn{!} \[ {\bf prpstn\_m\_rel}\\
                       lbl & h1\\
                       marg & h4 \] ,
                    \[ {\bf def\_q\_rel}\\
                       lbl & h10\\
                       arg0 & x8\\
                       rstr & h11\\
                       body & h12 \] ,
                    \[ {\bf \_dog\_n\_rel}\\
                       lbl & h7\\
                       arg0 & x8 \] ,\\
                    \[ {\bf \_try\_v\_rel}\\
                       lbl & h21\\
                       arg0 & e2\\
                       arg1 & x8\\
                       arg2 & h22 \] ,
                    \[ {\bf prpstn\_m\_rel}\\
		       lbl & h22\\
                       marg & h23 \] ,
		    \[ {\bf \_bark\_v\_rel}\\
                       lbl & h24\\
                       arg0 & e3\\
                       arg1 & x8 \]\ \tn{!}\q>\avmr\\
  hcons & \q<\tn{!} \[ {\it qeq}\\
                       harg & h4\\
                       larg & h21\] ,
		    \[ {\it qeq}\\
		       harg & h11\\
		       larg & h7 \] ,
		    \[ {\it qeq}\\
		       harg & h23\\
		       larg & h24 \]\ \tn{!}\q> \]
\end{avm}
}


%% \noindent
%% {\small
%% $\avmplus{\att{mrs}\\
%%           \attval{HOOK}{\avmplus{\attvaltyp{LTOP}{h1}\\
%%                                  \attvaltyp{INDEX}{e2}}}\\
%%           \attvallist{RELS}{
%% \avmplus{\att{prpstn}\\
%%          \attvaltyp{LBL}{h1}\\
%%          \attvaltyp{MARG}{h4}},
%% \avmplus{\att{def}\\
%%          \attvaltyp{LBL}{h10}\\
%%          \attvaltyp{ARG0}{x8}\\
%%          \attvaltyp{RSTR}{h11}\\
%%          \attvaltyp{BODY}{h12}},
%% \avmplus{\att{dog}\\
%%          \attvaltyp{LBL}{h7}\\
%%          \attvaltyp{ARG0}{x8}},
%% \avmplus{\att{try}\\
%%          \attvaltyp{LBL}{h21}\\
%%          \attvaltyp{ARG0}{e2}\\
%%          \attvaltyp{ARG1}{x8}\\
%%          \attvaltyp{ARG2}{h22}},
%% \avmplus{\att{prpstn}\\
%%          \attvaltyp{LBL}{h22}\\
%%          \attvaltyp{MARG}{h23}},
%% \avmplus{\att{bark}\\
%%          \attvaltyp{LBL}{h24}\\
%%          \attvaltyp{ARG0}{e3}\\
%%          \attvaltyp{ARG1}{x8}}}\\
%% \attvallist{HCONS}{
%%     \avmplus{\att{qeq}\\
%%              \attvaltyp{HARG}{h4}\\
%%              \attvaltyp{LARG}{h21}},
%%     \avmplus{\att{qeq}\\
%%              \attvaltyp{HARG}{h11}\\
%%              \attvaltyp{LARG}{h7}},
%%     \avmplus{\att{qeq}\\
%%              \attvaltyp{HARG}{h23}\\
%%              \attvaltyp{LARG}{h24}}}}$}
%% }

The construction of this representation is the result of the same general
principles of semantic composition presented above.  The {\it head-complement} rule 
unifies the verb {\it tried}'s constraints on
its complement with those of the verb phrase {\it to bark}, which results in the
identification of the {\sc xarg} value of that VP with the {\sc index} of the 
subject of {\it tried}.  The constraints on {\it tried}'s subject are propagated up
to the verb phrase {\it tried to bark} from the head-daughter {\it tried} by
the head-complement rule, and the semantics of this verb phrase preserve the
semantic properties of its daughters, including the desired re-entrancies with
the subject index.  Hence when the head-subject rule combines {\it the dog}
with {\it tried to bark}, the syntactic and semantic constraints of the noun
phrase are unified with those in the {\sc subj} attribute of the verb phrase,
resulting in the identification of the {\sc arg0} value introduced by {\it dog}
with the {\sc arg1} values in both {\bf \_try\_v\_rel} and {\bf \_bark\_v\_rel}.

\subsubsection{Raising}

The same {\sc xarg} attribute enables a straightforward representation of
raising phenomena, where a syntactic argument's semantic index is not linked
to any semantic argument position in a given lexical entry's semantic relation,
but is instead assigned to a role in the semantics of another syntactic 
argument of the lexical entry.  For example, the English verb {\it seem} can
have two syntactic arguments, an NP and a VP, as in {\it the dog seems to
bark}, but the semantics of {\it seem} introduces a one-argument relation
{\bf \_seem\_v\_rel} which takes a proposition.

Compare the following parse tree and lexical type definition to the ones
for the subject-control example with {\it try} above:

\enumsentence{\label{raising}
The dog seems to bark.

\begin{tree}
\psset{levelsep=.7in}
\br{S}{\br{\begin{avm}\[ index & \@1 \]\end{avm}}{
    \br{Det}{\lf{the}}
    \br{\begin{avm}\[ index & \@1\\ arg0 & \@1 \]\end{avm}}{\lf{dog}}}
       \br{\begin{avm}\[ subj & \q< \[ index & \@1 \] \q> \]\end{avm}}{
	 \br{V}{\lf{seems}}
	 \br{\begin{avm}\[ xarg & \@1 \]\end{avm}}{
	   \br{C}{\lf{to}}
	   \br{\begin{avm}\[ xarg & \@1\\ arg1 & \@1 \]\end{avm}}{
	     \lf{bark}}}}}
\end{tree}

%% \begin{tree}
%% \br{S}{\br{$\avmplus{\attval{INDEX}{\ind{1}}}$}
%%            {\br{Det}{\lf{the}}
%%             \br{$\avmplus{\attval{INDEX}{\ind{1}}\\
%%                           \attval{ARG0}{\ind{1}}}$}{\lf{dog}}}
%%        \br{$\avmplus{\attval{S.INDEX}{\ind{1}}}$}
%%            {\br{V}{\lf{seems}}
%%             \br{$\avmplus{\attval{XARG}{\ind{1}}}$}
%%                 {\br{C}{\lf{to}}
%%                  \br{$\avmplus{\attval{XARG}{\ind{1}}\\
%%                                \attval{ARG1}{\ind{1}}}$}{\lf{bark}}}}}
%% \end{tree}\\

}

\enumsentence{\label{srs}
Type for subject-raising verbs like {\it seem}

\begin{avm}
\[ {\it subj-rais-verb}\\
   subj & \q< \[ hook.index & \@1 \] \q>\\
   comps & \q< \[ hook & \[ ltop & \@2\\
                            xarg & \@1 \] \] \q>\\
   keyrel & \[ arg1 & \@2 \] \]
\end{avm}

%% $\avmplus{\att{\it subj-rais-verb}\\
%%           \attvallist{SUBJ}{
%%           \avmplus{\attval{HOOK.INDEX}{\ind{1}}}}\\

%%           \attvallist{COMPS}{
%%                       \avmplus{\attval{HOOK} {
%%                                        \avmplus{\attval{LTOP}{\ind{2}}\\
%%                                                  \attval{XARG}{\ind{1}}}}}}\\
%%           \attvallist{CONT.RELS}{
%%           \avmplus{\attval{ARG1}{\ind{2}}}}}$
}


The difference between the lexical entries for{\it seem} and {\it try} is
that the raising verb identifies the external argument of its VP complement
with its own subject's index, but then takes the proposition introduced by
the VP complement as its only argument, rather than assigning a second
semantic role to its own subject's index.

Object-raising lexical entries like the English {\it believe} instantiate
a lexical type similar to the one for subject-raising verbs, but identify 
the external argument of their VP complement with the index of their direct
object NP rather than that of their subject.  The relevant type is defined
as follows:

\enumsentence{\label{ors}
Type for object-raising verbs like {\it believe}

\begin{avm}
\[ {\it obj-rais-verb}\\
   subj & \q< \[ hook.index & \@1 \] \q>\\
   comps & \q< \[ hook.index & \@2 \] , \[ hook & \[ ltop & \@3\\
                                                     xarg & \@2 \]\] \q>\\
   keyrel & \[ arg1 & \@1\\
                     arg2 & \@3 \] \]
\end{avm}

%% $\avmplus{\att{\it obj-rais-verb}\\
%%           \attvallist{SUBJ}{
%%           \avmplus{\attval{HOOK.INDEX}{\ind{1}}}}\\

%%           \attvallist{COMPS}{
%%                       \avmplus{\attval{HOOK.INDEX}{\ind{2}}},
%%                       \avmplus{\attval{HOOK} {
%%                                        \avmplus{\attval{LTOP}{\ind{3}}\\
%%                                                 \attval{XARG}{\ind{2}}}}}}\\
%%           \attvallist{CONT.RELS}{
%%                            \avmplus{\attval{ARG1}{\ind{1}}\\
%%                                     \attval{ARG2}{\ind{3}}}}}$

}
\subsubsection{Expletives}

Constructions with expletive arguments are analyzed similarly to raising
constructions in that a lexical entry's syntactic argument introduces a
semantic index which does not appear as a role in the entry's semantic
predicate; indeed, that index appears in no relation in the semantics.
For example, the English sentence {\it it rained} is analyzed in the ERG
to have the semantics {\bf \_rain\_v\_rel}(e), with the lexical entry for
{\it rain} requiring its subject NP to be the expletive {\it it}.  The
lexical type for verbs like {\it rain} introduces the following constraints,
making use of an English-specific subtype of the Matrix {\it expl-ind} named
{\it it-ind} which is introduced by the relevant lexical entry for the NP
{\it it}.

\es{
\begin{avm}
\[ {\it expl-subj-verb}\\
   subj & \q< \[ hook.index & {\it it-ind} \] \q>\\
   keyrel & \[ arg0 & event \] \]
\end{avm}
}

%% $\avmplus{\att{\it expl-subj-verb}\\
%%           \attvallist{SUBJ}{
%%           \avmplus{\attval{HOOK.INDEX}{\it it-ind}}}\\
%%           \attvallist{CONT.RELS}{
%%           \avmplus{\attval{ARG0}{\it event}}}}$

The lexical entry for the English presentational {\it be} as in the example
{\it there is a dog here} requires its subject NP to have a semantic index
of a second type of expletive, called {\it there-ind}, which is introduced
by the relevant lexical entry for the NP {\it there}.  The following example
illustrates the interaction of expletives with object-raising predicates:

\enumsentence{\label{expl-rais}
The dog believes there to be a cat
}


% Postponed until the next version
%\section{Generation}
%\label{gen}

%[ ... What you need to do to make sure you can generate from your
%grammar, info on writing filters for semantically empty lexical items, ... ] 

\section{Verifying {\it mrs}s}
\label{verif}

The LKB provides several useful tools for ensuring that the {\it mrs} 
representations that are built with a Matrix-derived grammar are well formed.
These include batch tests for correctness of lexical entries and for the
fully scoped readings of sentences, as well as alternate views of the
{\it mrs}s for sentences.  The basic functionality is documented in 
\cite{Copestake:02}, so here we only highlight a few of the most common tests
that a grammar writer can use.

First, it is important that the lexical entries for the grammar are all
correctly defined, and one good batch test for this level of well-formedness
can be invoked from the ``LKB Top'' window, by selecting the ``Debug -- Check
lexicon'' menu item.  This will cause each lexical entry in the lexicon to be
expanded and checked, with helpful error messages printed if any flaws are
uncovered.

Second, the wellformedness of an {\it mrs} for a sentence can be checked by
making sure that the underspecified semantics constructed by the grammar
can be used to produce the right range of fully-scoped representations.  This
check can be invoked by left-clicking on one of the small parse trees that
appears after successfully parsing a sentence, and selecting the menu item
``Scoped MRS'', which will either bring up a window showing all of the 
possible fully-scoped realizations for the chosen sentence, or produce an
error message, often with helpful advice about what is wrong with the 
{\it mrs}.

A third useful view of an {\it mrs} can be invoked by again left-clicking
on one of the small parse trees, and selecting the menu item ``Dependencies'',
which will bring up a window showing a simplified view of the elementary
dependencies that can be computed from the input {\it mrs}.  If the color of
the text is blue, the argument links among predicates are all well-formed,
whereas if the text color in the window is yellow, at least one link is not
well-formed, and the line for the offending predicate is marked with an initial
vertical bar to help in diagnosis.  If the feature structure for the input
{\it mrs} has defined a circular linking structure (usually because of an
error in how the difference lists have been appended), then the text color
in this ``Dependencies'' window will be red.

\section{Sem-I: The Semantic Interface}
\label{semisec}

In order to make use of a semantic representation produced by a Matrix
grammar, an application developer will require a specification of how
to interpret the MRS structures, including documentation of the full
range of semantic relations and their arguments that the grammar introduces.
This specification, called the Semantic Interface (Sem-I) should be supplied
as part of each grammar, and will be the locus of some linguistic 
generalizations that one might have expected to see encoded directly in the
lexical type hierarchy or in lexical rules.

The Sem-I will consist of:
\begin{enumerate}
\item A manually-constructed
fully-documented database of all the relations introduced 
via constructions
and lexical types, and all the values which may appear on semantic 
features.  This is the meta-level Sem-I, and should be consistent across
languages: i.e., it forms part of the Matrix.
\item An automatically constructed database for the semantic information
pertaining to the open class words in each language.
This is the object-level Sem-I.  It is generated from the lexical
database for each grammar's lexicon.
\end{enumerate}

The Matrix will have associated code for generating the object-level
Sem-I, for validating the well-formedness of {\it mrs}s constructed by the 
grammar, and for employing Sem-I mappings in displaying MRS representations.

\subsection{The meta-level Sem-I}

This is a manually constructed,  fully documented database of all the 
relations introduced via constructions and lexical types, and all the values 
which may appear on semantic features.  Since this inventory is intended
to be consistent across languages, it forms part of the Matrix,
with links to language-specific examples that illustrate the use of these
relations and values for a particular grammar.

For example, a two-place relation introduced for noun-noun compounds in
an English grammar derived from the Matrix might appear in the meta-level 
Sem-I database as follows:

\vspace{0.1in}
\begin{tabular}{lllllllll}
PRED & ARG0 & & ARG1 & & ARG2 & & documentation & test suite\\
n-n-cmpnd\_rel & event & obl & 
index & obl & index & obl & \verb+<link to doc>+ & 
\verb+<ex. #>+
\end{tabular}
\vspace{0.1in}

\noindent
Here, the specification and instantiation of the types
of the ARG0, ARG1, ARG2 values may be done automatically, as will be the case
with the object-level Sem-I.  Each semantic role is identified as an
obligatory ('obl') or optional ('opt') argument of the predicate, with the 
expectation that more fine-grained distinctions may be needed later.
The documentation needs to explain the meaning and use
of the relation in as much detail as necessary for application
developers to decide how to treat it.  For instance, it would be 
important for a developer
to know whether {\bf n-n-cmpnd\_rel} was used for all 
noun-noun compounds in this grammar or only for some of them.

The meta-level Sem-I will also include specifications of semantic classes
to provide the basis for generalizations over thematic roles, and to enable
more mnemonic displays of MRS structures where the role names can be
specialized for these semantic classes.

\subsection{The object-level Sem-I}

Each grammar built using the Matrix should eventually include an automatically 
constructed database for the semantic information pertaining to the open class 
words in the language.  This is the object-level Sem-I, generated from the 
lexical database for each lexicon, with possible links to example sentences
that can be used for testing.

For example:\\

\noindent
{\small
\begin{tabular}{lllllllllll}
lexeme & string & PRED & ARG0 & & ARG1 & & ARG2 & & test suite eg & doc\\
chase\_v1 & chase & \_chase\_v\_rel & event & $+$ & index & $+$ & index & $+$ & Dogs chased cats & ``Doc''
\end{tabular}
}\\

In most cases there won't be a test suite example, but it may be that a
developer will add one to elucidate the use or distinction from another sense. 
Similarly, documentation may be added, but usually won't be.
The mechanism for adding documentation or test suite examples
must make it possible to autogenerate the links in this database.


\subsection{Thematic role mapping}

The Matrix uses a naming convention where role names of the form ARGn are 
used to identify
individual semantic arguments for predicates.  While this has proven to be
important in capturing generalizations about linking in the lexical type
hierarchy, it requires that other linguistic generalizations over thematic
roles be expressed instead in the Sem-I.

The approach planned for the Matrix is two-fold: a rich inventory
of word classes will be incrementally developed which will allow mapping
of ARGn relations into alternative inventories, and since this is
a long term project, it might be augmented with automatically
generated example sentences as a form of cheap documentation to convey the
intent of role assignments for any given predicate.

\subsection{Word classes}

Information about word classes should be added
incrementally to the lexical databases, without changing the grammars.
Documentation for these classes explains the notion of ARGn with respect to
that class, and
mappings to alternative thematic roles can be done on the basis of class 
membership.
Verb classes will be part of the Matrix Sem-I, since they are motivated by 
properties of the syntax-semantics interface.  In contrast, thematic role 
mappings are additions to the Sem-I which may be provided by developers of
a particular grammar.  The LKB software will support parameterized I/O routines
which will allow alternative thematic
role inventories to be supported if there is an available mapping.

For example, we can distinguish two classes of
English psych predicates: one in which the subject is
the experiencer of some emotion and the object the stimulus
(e.g., {\it Kim likes rabbits}) and the other in which the converse is true
(e.g. {\it Rabbits worry Kim}).  The lexical database entries
for {\it like} and {\it worry} can be enhanced to include their class.

A grammar developer wishing to add a semantic class to the Sem-I
should provide the following information:
\begin{enumerate}
\item Class name.
\item Class documentation, to include 
class membership criteria in the form of specific tests
and some specific exemplars.
\item Documentation of the ARGn behaviour of the class in the grammar.
\item A full list of all current lexical entries that are members
of the class.
\item A mapping between the ARGn for this class and any supported
thematic role sets.
\end{enumerate}

\subsection{Example sentences}
\label{sempref}

Automatically created examples might be used to
make the behaviour of lexical entries clearer to outside users.
The idea is to augment entries which have subcategorization with
some very coarse-grained selectional restriction information.  For instance:
\begin{itemize}
\item handle
\item event
\item non referential
\item referential
\begin{enumerate}
\item animate
\item non-animate physical
\item physical location
\item temporal location
\item abstract
\end{enumerate}
\end{itemize}
This would allow us to automatically construct standardized examples,
which can be used to illustrate ARGs etc.
For instance:
\begin{quote}
the person liked the thing\\
the person liked doing the thing\\
the person liked to do the thing
\end{quote}
where {\it person} is the standard term for animate entities and 
{\it thing} for non-animate ones.

The point about this approach is to make it reasonably intuitive for 
application developers while avoiding making the process of deciding on 
the semantic categories too
time-consuming.  It will also simplify the task of checking lexical
entries for over-generation, by allowing the grammar writer to scan the 
automatically generated examples for obviously incorrect sentences.

\section{Conclusion}

We have provided documentation here for the semantic types and their 
attributes that are used in the Matrix, as well as a discussion of how these 
types are used in semantic composition to produce well-formed meaning 
representations.  We illustrated the concepts primarily with examples from
English, but expect that the mechanisms introduced here should be useful in
building grammars for a wide variety of human languages.

\section{Acknowledgments}

This paper is an extension of work first reported in \namecite{Fli:Ben:03}.
We are grateful to the authors of {\it Minimal Recursion Semantics: An
Introduction}, from which this documentation draws heavily, and especially to
Ann Copestake for illumination on many issues presented here.  We also thank
the early adopters of the Matrix, in particular Melanie Siegel as principal
author of the wide-coverage JACY grammar of Japanese, and the developers of
the trail-blazing NorSource grammar of Norwegian: Lars Hellan, Dorothee
Beermann, and Petter Haugereid, as well as their colleague Kaja Borthen.  We
have received helpful questions and critique from the other members of the
Deep Thought consortium that helped to fund this work, especially from
Berthold Crysmann, currently developing a wide-coverage grammar of German for
the project; and also from colleagues working in the Norwegian LOGON machine
translation project.  As always, only the authors of this document can be
held responsible for any errors that survive here in spite of the excellent
counsel we received.

\bibliographystyle{robbib}
\bibliography{./userguide}

%\section*{Index}

\end{document}

